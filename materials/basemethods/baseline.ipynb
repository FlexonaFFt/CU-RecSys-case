{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08529c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flexonafft/curecsys/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/flexonafft/curecsys/myenv/lib/python3.9/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\" \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import threadpoolctl\n",
    "threadpoolctl.threadpool_limits(1, \"blas\")\n",
    "\n",
    "import implicit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from lightfm import LightFM\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict, Counter \n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from lightfm.data import Dataset as LightFMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a9a0a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сразу загружу данные\n",
    "train = pl.read_parquet(\"../../data/train.pq\")\n",
    "test = pl.read_parquet(\"../../data/test.pq\")\n",
    "\n",
    "train_items = set(train[\"item_id\"].unique())\n",
    "test_items = set(test[\"item_id\"].unique())\n",
    "cold_items = test_items - train_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d14111",
   "metadata": {},
   "source": [
    "### Метрики оценивания моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "903a280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validator(ABC):\n",
    "    def __init__(self, train: pd.DataFrame, test: pd.DataFrame, cold_items: set = None):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.cold_items = cold_items or set()\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self, predictions: Dict[int, List[int]]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        predictions: dict user_id -> list of recommended item_ids\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def recall_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        return len(set(y_true) & set(y_pred[:k])) / len(set(y_true)) if y_true else 0.0\n",
    "\n",
    "    def precision_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        return len(set(y_true) & set(y_pred[:k])) / k if y_true else 0.0\n",
    "\n",
    "    def hitrate_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        return 1.0 if len(set(y_true) & set(y_pred[:k])) > 0 else 0.0\n",
    "\n",
    "    def ndcg_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        dcg = 0.0\n",
    "        for i, item in enumerate(y_pred[:k]):\n",
    "            if item in y_true:\n",
    "                dcg += 1 / np.log2(i + 2)\n",
    "        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(y_true), k)))\n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    def mrr_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        for i, item in enumerate(y_pred[:k]):\n",
    "            if item in y_true:\n",
    "                return 1 / (i + 1)\n",
    "        return 0.0\n",
    "\n",
    "    def coverage(self, predictions: Dict[int, List[int]]) -> float:\n",
    "        all_pred_items = set(item for recs in predictions.values() for item in recs)\n",
    "        all_train_items = set(self.train[\"item_id\"].unique())\n",
    "        return len(all_pred_items) / len(all_train_items)\n",
    "\n",
    "    @staticmethod\n",
    "    def print_metrics(metrics: Dict[str, float]):\n",
    "        print(\"\\n=== Evaluation Results ===\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key:<15}: {value:.4f}\")\n",
    "        print(\"==========================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f99442",
   "metadata": {},
   "source": [
    "Стоит разделить валидацию на две версии, для сравнения. Моя гипотеза заключается в том, что совместная валидация warm и cold может быть не совсем честной. Например, если в тесте 90% warm и 10% cold, то Recall@10 в среднем будет определяться warm-айтемами. Модель может полностью «забыть» про cold items, но в отчёте всё равно будут хорошие цифры. Это вводит в заблуждение: кажется, что модель универсальная, хотя на самом деле cold-start не решён."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e509bb",
   "metadata": {},
   "source": [
    "#### Блок совместной валидации (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeb20ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointValidator(Validator):\n",
    "    def __init__(self, train: pl.DataFrame, test: pl.DataFrame, cold_items: set = None):\n",
    "        super().__init__(train, test, cold_items)\n",
    "        self.user2items = (\n",
    "            test.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_dict(as_series=False)\n",
    "        )\n",
    "        self.user2items = dict(zip(self.user2items[\"user_id\"], self.user2items[\"item_id\"]))\n",
    "\n",
    "    def evaluate(self, predictions: Dict[int, List[int]]) -> Dict[str, float]:\n",
    "        recalls, precisions, hits, ndcgs, mrrs = [], [], [], [], []\n",
    "        for user_id, y_pred in predictions.items():\n",
    "            y_true = self.user2items.get(user_id, [])\n",
    "            recalls.append(self.recall_at_k(y_true, y_pred))\n",
    "            precisions.append(self.precision_at_k(y_true, y_pred))\n",
    "            hits.append(self.hitrate_at_k(y_true, y_pred))\n",
    "            ndcgs.append(self.ndcg_at_k(y_true, y_pred))\n",
    "            mrrs.append(self.mrr_at_k(y_true, y_pred))\n",
    "        results = {\n",
    "            \"Recall@10\": np.mean(recalls),\n",
    "            \"Precision@10\": np.mean(precisions),\n",
    "            \"HitRate@10\": np.mean(hits),\n",
    "            \"NDCG@10\": np.mean(ndcgs),\n",
    "            \"MRR@10\": np.mean(mrrs),\n",
    "            \"Coverage\": self.coverage(predictions),\n",
    "        }\n",
    "        self.print_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a671ccdf",
   "metadata": {},
   "source": [
    "#### Разделенная валидация (cold vs warm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae07589",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitValidator(Validator):\n",
    "    def __init__(self, train: pl.DataFrame, test: pl.DataFrame, cold_items: set = None):\n",
    "        super().__init__(train, test, cold_items)\n",
    "        self.user2items = (\n",
    "            test.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_dict(as_series=False)\n",
    "        )\n",
    "        self.user2items = dict(zip(self.user2items[\"user_id\"], self.user2items[\"item_id\"]))\n",
    "\n",
    "    def evaluate(self, predictions: Dict[int, List[int]]) -> Dict[str, float]:\n",
    "        results = {}\n",
    "        for subset in [\"cold\", \"warm\"]:\n",
    "            recalls, precisions, hits, ndcgs, mrrs = [], [], [], [], []\n",
    "            for user_id, y_pred in predictions.items():\n",
    "                y_true = self.user2items.get(user_id, [])\n",
    "                if not y_true:\n",
    "                    continue\n",
    "                if subset == \"cold\":\n",
    "                    y_true = [i for i in y_true if i in self.cold_items]\n",
    "                elif subset == \"warm\":\n",
    "                    y_true = [i for i in y_true if i not in self.cold_items]\n",
    "                if not y_true:\n",
    "                    continue\n",
    "                recalls.append(self.recall_at_k(y_true, y_pred))\n",
    "                precisions.append(self.precision_at_k(y_true, y_pred))\n",
    "                hits.append(self.hitrate_at_k(y_true, y_pred))\n",
    "                ndcgs.append(self.ndcg_at_k(y_true, y_pred))\n",
    "                mrrs.append(self.mrr_at_k(y_true, y_pred))\n",
    "            results[f\"Recall@10_{subset}\"] = np.mean(recalls) if recalls else 0.0\n",
    "            results[f\"Precision@10_{subset}\"] = np.mean(precisions) if precisions else 0.0\n",
    "            results[f\"HitRate@10_{subset}\"] = np.mean(hits) if hits else 0.0\n",
    "            results[f\"NDCG@10_{subset}\"] = np.mean(ndcgs) if ndcgs else 0.0\n",
    "            results[f\"MRR@10_{subset}\"] = np.mean(mrrs) if mrrs else 0.0\n",
    "        results[\"Coverage\"] = self.coverage(predictions)\n",
    "        self.print_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a0360",
   "metadata": {},
   "source": [
    "#### Вспомогательные функции для удобного представления результатов\n",
    "\n",
    "Необходимы для составления табличек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae9dd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shorten_list(lst, max_len=10):\n",
    "    \"\"\"Обрезает длинные списки для красивого вывода\"\"\"\n",
    "    if lst is None:\n",
    "        return []\n",
    "    return lst[:max_len] if len(lst) > max_len else lst\n",
    "\n",
    "def show_predictions(models: dict, data: pl.DataFrame, n=5, verbose=True, is_val=False):\n",
    "    df = data.sample(n).select([\"user_id\", \"item_id\"])\n",
    "    if is_val:\n",
    "        df = df.rename({\"item_id\": \"true_items\"})\n",
    "\n",
    "    # добавляем предсказания\n",
    "    for name, preds in models.items():\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"user_id\").map_elements(\n",
    "                lambda u: _shorten_list(preds.get(u, [])), \n",
    "                return_dtype=pl.List(pl.Int64)\n",
    "            ).alias(name)\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        print(df.shape)\n",
    "        print(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def val_predictions(models: dict, val: pl.DataFrame, validator: Validator, k: int = 10, verbose: bool = True):\n",
    "    results = []\n",
    "    user2items = (\n",
    "        val.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_dict(as_series=False)\n",
    "    )\n",
    "    user2items = dict(zip(user2items[\"user_id\"], user2items[\"item_id\"]))\n",
    "\n",
    "    for model_name, preds in models.items():\n",
    "        recalls, precisions, hits, ndcgs, mrrs = [], [], [], [], []\n",
    "        for u, y_true in user2items.items():\n",
    "            y_pred = preds.get(u, [])\n",
    "            recalls.append(validator.recall_at_k(y_true, y_pred, k))\n",
    "            precisions.append(validator.precision_at_k(y_true, y_pred, k))\n",
    "            hits.append(validator.hitrate_at_k(y_true, y_pred, k))\n",
    "            ndcgs.append(validator.ndcg_at_k(y_true, y_pred, k))\n",
    "            mrrs.append(validator.mrr_at_k(y_true, y_pred, k))\n",
    "        metrics = {\n",
    "            \"model\": model_name,\n",
    "            \"Recall@10\": np.mean(recalls),\n",
    "            \"Precision@10\": np.mean(precisions),\n",
    "            \"HitRate@10\": np.mean(hits),\n",
    "            \"NDCG@10\": np.mean(ndcgs),\n",
    "            \"MRR@10\": np.mean(mrrs),\n",
    "            \"Coverage\": validator.coverage(preds),\n",
    "        }\n",
    "        results.append(metrics)\n",
    "\n",
    "    df = pl.DataFrame(results)\n",
    "    if verbose:\n",
    "        print(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372f961c",
   "metadata": {},
   "source": [
    "### ALS и LightFM \n",
    "\n",
    "#### Определение базового класса для методов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "202cdc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender(ABC):\n",
    "    @abstractmethod\n",
    "    def fit(self, train_data: pl.DataFrame):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self, users: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3906d268",
   "metadata": {},
   "source": [
    "#### ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d1e4776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALSRecommender(Recommender):\n",
    "    def __init__(self, factors=32, iterations=10, regularization=0.01, num_threads=4):\n",
    "        self.factors = factors\n",
    "        self.iterations = iterations\n",
    "        self.regularization = regularization\n",
    "        self.num_threads = num_threads\n",
    "        self.model = None\n",
    "        self.user_map = None\n",
    "        self.item_map = None\n",
    "        self.items = None\n",
    "        self.users = None\n",
    "        self.popular = None\n",
    "        self.user_items = None\n",
    "        \n",
    "    def fit(self, train_data: pl.DataFrame):\n",
    "        # Convert to pandas for compatibility\n",
    "        train_pd = train_data.to_pandas()\n",
    "        \n",
    "        # Create mappings\n",
    "        self.users = sorted(train_pd['user_id'].unique())\n",
    "        self.items = sorted(train_pd['item_id'].unique())\n",
    "        self.user_map = {u: i for i, u in enumerate(self.users)}\n",
    "        self.item_map = {it: j for j, it in enumerate(self.items)}\n",
    "        \n",
    "        # Compute popular items for fallback\n",
    "        item_counts = train_data.group_by(\"item_id\").agg(pl.len().alias(\"count\")).sort(\"count\", descending=True)\n",
    "        self.popular = item_counts[\"item_id\"].head(10).to_list()\n",
    "        \n",
    "        # Create sparse matrix (rows=users, cols=items)  # <-- FIXED HERE\n",
    "        rows = [self.user_map[u] for u in train_pd['user_id']]\n",
    "        cols = [self.item_map[it] for it in train_pd['item_id']]\n",
    "        data = np.ones(len(train_pd))\n",
    "        user_items = csr_matrix((data, (rows, cols)), shape=(len(self.users), len(self.items)))\n",
    "        \n",
    "        # Train ALS\n",
    "        self.model = AlternatingLeastSquares(\n",
    "            factors=self.factors,\n",
    "            iterations=self.iterations,\n",
    "            regularization=self.regularization,\n",
    "            num_threads=self.num_threads\n",
    "        )\n",
    "        self.model.fit(user_items)\n",
    "        self.user_items = user_items  # <-- No transpose needed now\n",
    "        \n",
    "    def predict(self, users: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        predictions = {}\n",
    "        for user in tqdm(users, desc=\"ALS predictions\"):\n",
    "            if user not in self.user_map:\n",
    "                predictions[user] = self.popular\n",
    "                continue\n",
    "            userid = self.user_map[user]\n",
    "            try:\n",
    "                # Ensure user_items slice is valid\n",
    "                user_interactions = self.user_items[userid]\n",
    "                recs, _ = self.model.recommend(\n",
    "                    userid, \n",
    "                    user_interactions, \n",
    "                    N=k, \n",
    "                    filter_already_liked_items=True\n",
    "                )\n",
    "                predictions[user] = [self.items[r] for r in recs]\n",
    "            except IndexError:\n",
    "                # Fallback to popular items if index error occurs\n",
    "                predictions[user] = self.popular\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc5e5e",
   "metadata": {},
   "source": [
    "#### LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b710272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightFMRecommender(Recommender):\n",
    "    def __init__(self, no_components=16, loss='warp', learning_rate=0.05, num_threads=1):  # Changed no_components to 16, num_threads to 1\n",
    "        self.no_components = no_components\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_threads = num_threads\n",
    "        self.model = None\n",
    "        self.dataset = None\n",
    "        self.user_map = None\n",
    "        self.item_map = None\n",
    "        self.items = None\n",
    "        self.popular = None\n",
    "        \n",
    "    def fit(self, train_data: pl.DataFrame):\n",
    "        # Convert to pandas\n",
    "        train_pd = train_data.to_pandas()\n",
    "        \n",
    "        # Create mappings\n",
    "        users = sorted(train_pd['user_id'].unique())\n",
    "        self.items = sorted(train_pd['item_id'].unique())\n",
    "        self.user_map = {u: i for i, u in enumerate(users)}\n",
    "        self.item_map = {it: j for j, it in enumerate(self.items)}\n",
    "        \n",
    "        # Compute popular items for fallback\n",
    "        item_counts = train_data.group_by(\"item_id\").agg(pl.len().alias(\"count\")).sort(\"count\", descending=True)\n",
    "        self.popular = item_counts[\"item_id\"].head(10).to_list()\n",
    "        \n",
    "        # Prepare LightFM data\n",
    "        self.dataset = LightFMDataset()\n",
    "        self.dataset.fit(users=users, items=self.items)\n",
    "        \n",
    "        # Build interactions incrementally to save memory\n",
    "        interactions = None\n",
    "        batch_size = 100000  # Process 100k interactions at a time\n",
    "        for start in tqdm(range(0, len(train_pd), batch_size), desc=\"Building interactions\"):\n",
    "            batch = train_pd[start:start + batch_size][['user_id', 'item_id']]\n",
    "            batch_interactions = self.dataset.build_interactions(zip(batch['user_id'], batch['item_id']))[0]\n",
    "            if interactions is None:\n",
    "                interactions = batch_interactions\n",
    "            else:\n",
    "                interactions += batch_interactions\n",
    "        \n",
    "        # Train LightFM\n",
    "        self.model = LightFM(\n",
    "            no_components=self.no_components,\n",
    "            loss=self.loss,\n",
    "            learning_rate=self.learning_rate,\n",
    "            learning_schedule='adagrad'\n",
    "        )\n",
    "        self.model.fit(interactions, epochs=5, num_threads=self.num_threads)  # Reduced to 5 epochs\n",
    "        \n",
    "    def predict(self, users: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        predictions = {}\n",
    "        item_ids_mapped = np.arange(len(self.items))\n",
    "        \n",
    "        # Warm users\n",
    "        warm_users = [u for u in users if u in self.user_map]\n",
    "        userids_mapped = np.array([self.user_map[u] for u in warm_users])\n",
    "        \n",
    "        if len(userids_mapped) > 0:\n",
    "            # Batch predict with smaller batches to avoid memory issues\n",
    "            batch_size = 1000  # Predict for 1000 users at a time\n",
    "            for start in tqdm(range(0, len(userids_mapped), batch_size), desc=\"LightFM predictions\"):\n",
    "                batch_users = userids_mapped[start:start + batch_size]\n",
    "                batch_warm_users = warm_users[start:start + batch_size]\n",
    "                user_ids_arr = np.repeat(batch_users, len(self.items))\n",
    "                item_ids_arr = np.tile(item_ids_mapped, len(batch_users))\n",
    "                scores_flat = self.model.predict(user_ids_arr, item_ids_arr)\n",
    "                scores = scores_flat.reshape(len(batch_users), len(self.items))\n",
    "                top_items = np.argsort(-scores, axis=1)[:, :k]\n",
    "                for idx, u in enumerate(batch_warm_users):\n",
    "                    recs = [self.items[r] for r in top_items[idx]]\n",
    "                    predictions[u] = recs\n",
    "        \n",
    "        # Cold users\n",
    "        for user in users:\n",
    "            if user not in predictions:\n",
    "                predictions[user] = self.popular\n",
    "                \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a837776d",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1bfa829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train users: 349719, items: 31300\n",
      "Test users: 185828, items: 27367\n",
      "Cold items: 1775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.06it/s]\n",
      "Building interactions: 100%|██████████| 12/12 [00:01<00:00,  9.82it/s]\n",
      "ALS predictions: 100%|██████████| 185828/185828 [00:13<00:00, 14203.53it/s]\n",
      "LightFM predictions: 100%|██████████| 136/136 [11:09<00:00,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Рекомендации моделей:\n",
      "(5, 4)\n",
      "shape: (5, 4)\n",
      "┌─────────────────────────────────┬─────────┬────────────────────────┬─────────────────────────┐\n",
      "│ user_id                         ┆ item_id ┆ ALS                    ┆ LightFM                 │\n",
      "│ ---                             ┆ ---     ┆ ---                    ┆ ---                     │\n",
      "│ str                             ┆ i64     ┆ list[i64]              ┆ list[i64]               │\n",
      "╞═════════════════════════════════╪═════════╪════════════════════════╪═════════════════════════╡\n",
      "│ a4e96406524159b6861d75b2e96bcc… ┆ 14656   ┆ [1375, 2672, … 25039]  ┆ [15514, 18150, … 28386] │\n",
      "│ 34d07ca7cb4b1f164e160affaac11b… ┆ 4126    ┆ [27524, 9722, … 1274]  ┆ [18150, 15514, … 9147]  │\n",
      "│ 55e247ca12f56ebab6ad9fa125e9f6… ┆ 3513    ┆ []                     ┆ []                      │\n",
      "│ c3ce4d3f53fdc6f4448748f34142d3… ┆ 7851    ┆ [11592, 4917, … 29055] ┆ [18150, 33851, … 24417] │\n",
      "│ 73ac133c8a3d1534afb28e66579b3f… ┆ 18150   ┆ []                     ┆ []                      │\n",
      "└─────────────────────────────────┴─────────┴────────────────────────┴─────────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train users: {train['user_id'].n_unique()}, items: {len(train_items)}\")\n",
    "print(f\"Test users: {test['user_id'].n_unique()}, items: {len(test_items)}\")\n",
    "print(f\"Cold items: {len(cold_items)}\")\n",
    "user_ids = test[\"user_id\"].unique().to_list()\n",
    "train_sample = train.sample(fraction=0.1)\n",
    "\n",
    "als_recommender = ALSRecommender(factors=32, iterations=10, regularization=0.01, num_threads=4)\n",
    "lightfm_recommender = LightFMRecommender(no_components=32, loss='warp', learning_rate=0.05, num_threads=4)\n",
    "als_recommender.fit(train_sample)\n",
    "lightfm_recommender.fit(train_sample)\n",
    "\n",
    "test_users = test[\"user_id\"].unique().to_list()\n",
    "pred_als = als_recommender.predict(test_users)\n",
    "pred_lightfm = lightfm_recommender.predict(test_users)\n",
    "\n",
    "models = {\n",
    "    \"ALS\": pred_als,\n",
    "    \"LightFM\": pred_lightfm,\n",
    "}\n",
    "\n",
    "\n",
    "print(\"\\nРекомендации моделей:\")\n",
    "train_df = show_predictions(models, train, n=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6af9b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Статистика по метрикам для каждой модели:\n",
      "shape: (2, 7)\n",
      "┌─────────┬───────────┬──────────────┬────────────┬──────────┬──────────┬──────────┐\n",
      "│ model   ┆ Recall@10 ┆ Precision@10 ┆ HitRate@10 ┆ NDCG@10  ┆ MRR@10   ┆ Coverage │\n",
      "│ ---     ┆ ---       ┆ ---          ┆ ---        ┆ ---      ┆ ---      ┆ ---      │\n",
      "│ str     ┆ f64       ┆ f64          ┆ f64        ┆ f64      ┆ f64      ┆ f64      │\n",
      "╞═════════╪═══════════╪══════════════╪════════════╪══════════╪══════════╪══════════╡\n",
      "│ ALS     ┆ 0.036348  ┆ 0.033381     ┆ 0.217454   ┆ 0.043746 ┆ 0.082427 ┆ 0.005048 │\n",
      "│ LightFM ┆ 0.03382   ┆ 0.028511     ┆ 0.183325   ┆ 0.036579 ┆ 0.062809 ┆ 0.029936 │\n",
      "└─────────┴───────────┴──────────────┴────────────┴──────────┴──────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nСтатистика по метрикам для каждой модели:\")\n",
    "validator = JointValidator(train, test, cold_items)\n",
    "metrics_df = val_predictions(models, test, validator, k=10, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
