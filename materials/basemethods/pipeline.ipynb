{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9713ced",
   "metadata": {},
   "source": [
    "# Базовый солюшн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7a7cd139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "80ea04ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сразу загружу данные\n",
    "train = pl.read_parquet(\"../../data/train.pq\")\n",
    "test = pl.read_parquet(\"../../data/test.pq\")\n",
    "\n",
    "train_items = set(train[\"item_id\"].unique())\n",
    "test_items = set(test[\"item_id\"].unique())\n",
    "cold_items = test_items - train_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a5282",
   "metadata": {},
   "source": [
    "### Метрики оценивания моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2bc75e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validator(ABC):\n",
    "    def __init__(self, train: pd.DataFrame, test: pd.DataFrame, cold_items: set = None):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.cold_items = cold_items or set()\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self, predictions: Dict[int, List[int]]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        predictions: dict user_id -> list of recommended item_ids\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def recall_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        return len(set(y_true) & set(y_pred[:k])) / len(set(y_true)) if y_true else 0.0\n",
    "\n",
    "    def precision_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        return len(set(y_true) & set(y_pred[:k])) / k if y_true else 0.0\n",
    "\n",
    "    def hitrate_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        return 1.0 if len(set(y_true) & set(y_pred[:k])) > 0 else 0.0\n",
    "\n",
    "    def ndcg_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        dcg = 0.0\n",
    "        for i, item in enumerate(y_pred[:k]):\n",
    "            if item in y_true:\n",
    "                dcg += 1 / np.log2(i + 2)\n",
    "        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(y_true), k)))\n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    def mrr_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        for i, item in enumerate(y_pred[:k]):\n",
    "            if item in y_true:\n",
    "                return 1 / (i + 1)\n",
    "        return 0.0\n",
    "\n",
    "    def coverage(self, predictions: Dict[int, List[int]]) -> float:\n",
    "        all_pred_items = set(item for recs in predictions.values() for item in recs)\n",
    "        all_train_items = set(self.train[\"item_id\"].unique())\n",
    "        return len(all_pred_items) / len(all_train_items)\n",
    "\n",
    "    @staticmethod\n",
    "    def print_metrics(metrics: Dict[str, float]):\n",
    "        print(\"\\n=== Evaluation Results ===\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key:<15}: {value:.4f}\")\n",
    "        print(\"==========================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01662f85",
   "metadata": {},
   "source": [
    "Стоит разделить валидацию на две версии, для сравнения. Моя гипотеза заключается в том, что совместная валидация warm и cold может быть не совсем честной. Например, если в тесте 90% warm и 10% cold, то Recall@10 в среднем будет определяться warm-айтемами. Модель может полностью «забыть» про cold items, но в отчёте всё равно будут хорошие цифры. Это вводит в заблуждение: кажется, что модель универсальная, хотя на самом деле cold-start не решён."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7d849",
   "metadata": {},
   "source": [
    "#### Блок совместной валидации (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "64a70b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointValidator(Validator):\n",
    "    def __init__(self, train: pl.DataFrame, test: pl.DataFrame, cold_items: set = None):\n",
    "        super().__init__(train, test, cold_items)\n",
    "        self.user2items = (\n",
    "            test.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_dict(as_series=False)\n",
    "        )\n",
    "        self.user2items = dict(zip(self.user2items[\"user_id\"], self.user2items[\"item_id\"]))\n",
    "\n",
    "    def evaluate(self, predictions: Dict[int, List[int]]) -> Dict[str, float]:\n",
    "        recalls, precisions, hits, ndcgs, mrrs = [], [], [], [], []\n",
    "        for user_id, y_pred in predictions.items():\n",
    "            y_true = self.user2items.get(user_id, [])\n",
    "            recalls.append(self.recall_at_k(y_true, y_pred))\n",
    "            precisions.append(self.precision_at_k(y_true, y_pred))\n",
    "            hits.append(self.hitrate_at_k(y_true, y_pred))\n",
    "            ndcgs.append(self.ndcg_at_k(y_true, y_pred))\n",
    "            mrrs.append(self.mrr_at_k(y_true, y_pred))\n",
    "        results = {\n",
    "            \"Recall@10\": np.mean(recalls),\n",
    "            \"Precision@10\": np.mean(precisions),\n",
    "            \"HitRate@10\": np.mean(hits),\n",
    "            \"NDCG@10\": np.mean(ndcgs),\n",
    "            \"MRR@10\": np.mean(mrrs),\n",
    "            \"Coverage\": self.coverage(predictions),\n",
    "        }\n",
    "        self.print_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fbe954",
   "metadata": {},
   "source": [
    "#### Разделенная валидация (cold vs warm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fceeeaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitValidator(Validator):\n",
    "    def __init__(self, train: pl.DataFrame, test: pl.DataFrame, cold_items: set = None):\n",
    "        super().__init__(train, test, cold_items)\n",
    "        self.user2items = (\n",
    "            test.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_dict(as_series=False)\n",
    "        )\n",
    "        self.user2items = dict(zip(self.user2items[\"user_id\"], self.user2items[\"item_id\"]))\n",
    "\n",
    "    def evaluate(self, predictions: Dict[int, List[int]]) -> Dict[str, float]:\n",
    "        results = {}\n",
    "        for subset in [\"cold\", \"warm\"]:\n",
    "            recalls, precisions, hits, ndcgs, mrrs = [], [], [], [], []\n",
    "            for user_id, y_pred in predictions.items():\n",
    "                y_true = self.user2items.get(user_id, [])\n",
    "                if not y_true:\n",
    "                    continue\n",
    "                if subset == \"cold\":\n",
    "                    y_true = [i for i in y_true if i in self.cold_items]\n",
    "                elif subset == \"warm\":\n",
    "                    y_true = [i for i in y_true if i not in self.cold_items]\n",
    "                if not y_true:\n",
    "                    continue\n",
    "                recalls.append(self.recall_at_k(y_true, y_pred))\n",
    "                precisions.append(self.precision_at_k(y_true, y_pred))\n",
    "                hits.append(self.hitrate_at_k(y_true, y_pred))\n",
    "                ndcgs.append(self.ndcg_at_k(y_true, y_pred))\n",
    "                mrrs.append(self.mrr_at_k(y_true, y_pred))\n",
    "            results[f\"Recall@10_{subset}\"] = np.mean(recalls) if recalls else 0.0\n",
    "            results[f\"Precision@10_{subset}\"] = np.mean(precisions) if precisions else 0.0\n",
    "            results[f\"HitRate@10_{subset}\"] = np.mean(hits) if hits else 0.0\n",
    "            results[f\"NDCG@10_{subset}\"] = np.mean(ndcgs) if ndcgs else 0.0\n",
    "            results[f\"MRR@10_{subset}\"] = np.mean(mrrs) if mrrs else 0.0\n",
    "        results[\"Coverage\"] = self.coverage(predictions)\n",
    "        self.print_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b15366b",
   "metadata": {},
   "source": [
    "### Базовые методы "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "17f65c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemPopRecSys:\n",
    "    def __init__(self, train: pd.DataFrame):\n",
    "        self.train = train\n",
    "        self.popular_items = None\n",
    "\n",
    "    def fit(self):\n",
    "        # Считаем популярность айтемов по числу взаимодействий\n",
    "        popularity = (self.train.group_by(\"item_id\").len().sort(\"len\", descending=True))\n",
    "        self.popular_items = popularity[\"item_id\"].to_list()\n",
    "\n",
    "    def recommend(self, user_ids: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        if self.popular_items is None:\n",
    "            raise ValueError(\"Model is not fitted. Call fit() before recommend().\")\n",
    "        preds = {u: self.popular_items[:k] for u in user_ids}\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8239bb1",
   "metadata": {},
   "source": [
    "##### Проблематика ItemPopRecSys и cold-start\n",
    "\n",
    "* `ItemPopRecSys` - рекомендует только книги, которые были в train.\n",
    "* Но cold items (которые есть только в test) он никогда не предложит.\n",
    "* __Это особенность__ - все чисто коллаборативные методы не умеют cold-start без дополнительных фичей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ff1a8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserKNNRecSys:\n",
    "    def __init__(self, train: pl.DataFrame, k_neighbors: int = 20):\n",
    "        self.train = train\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.user2items = None\n",
    "\n",
    "    def fit(self):\n",
    "        self.user2items = (self.train.group_by(\"user_id\").agg(pl.col(\"item_id\").unique()).to_dict(as_series=False))\n",
    "        self.user2items = {u: set(items) for u, items in zip(self.user2items[\"user_id\"], self.user2items[\"item_id\"])}\n",
    "\n",
    "    def recommend(self, user_ids: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        preds = {}\n",
    "        for u in user_ids:\n",
    "            if u not in self.user2items:\n",
    "                preds[u] = []\n",
    "                continue\n",
    "            # считаем схожесть с другими пользователями\n",
    "            scores = defaultdict(int)\n",
    "            for v, items in self.user2items.items():\n",
    "                if u == v:\n",
    "                    continue\n",
    "                common = len(self.user2items[u] & items)\n",
    "                if common > 0:\n",
    "                    for it in items:\n",
    "                        if it not in self.user2items[u]:\n",
    "                            scores[it] += common\n",
    "            recs = sorted(scores, key=scores.get, reverse=True)[:k]\n",
    "            preds[u] = recs\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff180d52",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "17c48852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemKNNRecSys:\n",
    "    def __init__(self, train: pl.DataFrame, k_neighbors: int = 20):\n",
    "        self.train = train\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.item2users = None\n",
    "\n",
    "    def fit(self):\n",
    "        self.user2items = (self.train.group_by(\"user_id\").agg(pl.col(\"item_id\").unique()).to_dict(as_series=False))\n",
    "        self.user2items = {u: set(items) for u, items in zip(self.user2items[\"user_id\"], self.user2items[\"item_id\"])}\n",
    "\n",
    "    def recommend(self, user_ids: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        df = self.train.to_pandas()\n",
    "        user2items = df.groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "        preds = {}\n",
    "        for u in user_ids:\n",
    "            if u not in user2items:\n",
    "                preds[u] = []\n",
    "                continue\n",
    "            scores = defaultdict(int)\n",
    "            for it in user2items[u]:\n",
    "                if it not in self.item2users:\n",
    "                    continue\n",
    "                users_who_liked = self.item2users[it]\n",
    "                for other_item, other_users in self.item2users.items():\n",
    "                    if other_item in user2items[u]:\n",
    "                        continue\n",
    "                    common = len(users_who_liked & other_users)\n",
    "                    if common > 0:\n",
    "                        scores[other_item] += common\n",
    "            recs = sorted(scores, key=scores.get, reverse=True)[:k]\n",
    "            preds[u] = recs\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd395434",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bc71207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MostRecentItemsRecSys:\n",
    "    def __init__(self, items_metadata: pl.DataFrame, time_col: str = \"timestamp\"):\n",
    "        self.items_metadata = items_metadata\n",
    "        self.time_col = time_col\n",
    "        self.sorted_items = None\n",
    "\n",
    "    def fit(self):\n",
    "        if self.time_col not in self.items_metadata.columns:\n",
    "            raise ValueError(\n",
    "                f\"Column '{self.time_col}' not found. \"\n",
    "                f\"Available columns: {self.items_metadata.columns}\"\n",
    "            )\n",
    "        self.sorted_items = (\n",
    "            self.items_metadata\n",
    "            .sort(self.time_col, descending=True)[\"item_id\"]\n",
    "            .to_list()\n",
    "        )\n",
    "\n",
    "    def recommend(self, user_ids: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        if self.sorted_items is None:\n",
    "            raise ValueError(\"Model is not fitted. Call fit() before recommend().\")\n",
    "        return {u: self.sorted_items[:k] for u in user_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2919327b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f4c139f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomColdRecSys:\n",
    "    def __init__(self, cold_items: List[int], seed: int = 42):\n",
    "        self.cold_items = cold_items\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def recommend(self, user_ids: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        return {\n",
    "            u: self.rng.choice(self.cold_items, size=min(k, len(self.cold_items)), replace=False).tolist()\n",
    "            for u in user_ids\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cc2924",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3bc68b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridPopColdRecSys:\n",
    "    def __init__(self, train: pl.DataFrame, cold_items: List[int]):\n",
    "        self.train = train\n",
    "        self.cold_items = cold_items\n",
    "        self.popular_items = None\n",
    "\n",
    "    def fit(self):\n",
    "        popularity = (\n",
    "            self.train.group_by(\"item_id\").len().sort(\"len\", descending=True)\n",
    "        )\n",
    "        self.popular_items = popularity[\"item_id\"].to_list()\n",
    "\n",
    "    def recommend(self, user_ids: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        preds = {}\n",
    "        for u in user_ids:\n",
    "            recs = self.popular_items[: k // 2]\n",
    "            if self.cold_items:\n",
    "                recs += list(np.random.choice(self.cold_items, size=min(k - len(recs), len(self.cold_items)), replace=False))\n",
    "            preds[u] = recs[:k]\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb8b02",
   "metadata": {},
   "source": [
    "#### Вспомогательные функции для удобного представления результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ce3f56bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shorten_list(lst, max_len=10):\n",
    "    \"\"\"Обрезает длинные списки для красивого вывода\"\"\"\n",
    "    if lst is None:\n",
    "        return []\n",
    "    return lst[:max_len] if len(lst) > max_len else lst\n",
    "\n",
    "def show_predictions(models: dict, data: pl.DataFrame, n=5, verbose=True, is_val=False):\n",
    "    # базовые колонки\n",
    "    df = data.sample(n).select([\"user_id\", \"item_id\"])\n",
    "    if is_val:\n",
    "        df = df.rename({\"item_id\": \"true_items\"})\n",
    "\n",
    "    # добавляем предсказания\n",
    "    for name, preds in models.items():\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"user_id\").map_elements(\n",
    "                lambda u: _shorten_list(preds.get(u, [])), \n",
    "                return_dtype=pl.List(pl.Int64)\n",
    "            ).alias(name)\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        print(df.shape)\n",
    "        print(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def val_predictions(models: dict, val: pl.DataFrame, validator: Validator, k: int = 10, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    models: dict[str, dict[user_id -> list[item_id]]]\n",
    "    val: validation DataFrame\n",
    "    validator: объект класса Validator с методами метрик\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    user2items = (\n",
    "        val.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_dict(as_series=False)\n",
    "    )\n",
    "    user2items = dict(zip(user2items[\"user_id\"], user2items[\"item_id\"]))\n",
    "\n",
    "    for model_name, preds in models.items():\n",
    "        recalls, precisions, hits, ndcgs, mrrs = [], [], [], [], []\n",
    "        for u, y_true in user2items.items():\n",
    "            y_pred = preds.get(u, [])\n",
    "            recalls.append(validator.recall_at_k(y_true, y_pred, k))\n",
    "            precisions.append(validator.precision_at_k(y_true, y_pred, k))\n",
    "            hits.append(validator.hitrate_at_k(y_true, y_pred, k))\n",
    "            ndcgs.append(validator.ndcg_at_k(y_true, y_pred, k))\n",
    "            mrrs.append(validator.mrr_at_k(y_true, y_pred, k))\n",
    "        metrics = {\n",
    "            \"model\": model_name,\n",
    "            \"Recall@10\": np.mean(recalls),\n",
    "            \"Precision@10\": np.mean(precisions),\n",
    "            \"HitRate@10\": np.mean(hits),\n",
    "            \"NDCG@10\": np.mean(ndcgs),\n",
    "            \"MRR@10\": np.mean(mrrs),\n",
    "            \"Coverage\": validator.coverage(preds),\n",
    "        }\n",
    "        results.append(metrics)\n",
    "\n",
    "    df = pl.DataFrame(results)\n",
    "    if verbose:\n",
    "        print(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5e1a315e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train users: 349719, items: 31300\n",
      "Test users: 185828, items: 27367\n",
      "Cold items: 1775\n",
      "Train users: 349719, items: 31300\n",
      "(5, 5)\n",
      "shape: (5, 5)\n",
      "┌──────────────────────────┬─────────┬─────────────────┬─────────────────┬─────────────────────────┐\n",
      "│ user_id                  ┆ item_id ┆ Popularity      ┆ Recent          ┆ RandomCold              │\n",
      "│ ---                      ┆ ---     ┆ ---             ┆ ---             ┆ ---                     │\n",
      "│ str                      ┆ i64     ┆ list[i64]       ┆ list[i64]       ┆ list[i64]               │\n",
      "╞══════════════════════════╪═════════╪═════════════════╪═════════════════╪═════════════════════════╡\n",
      "│ 1e27aefa36f61909617bb6f7 ┆ 1166    ┆ [4058, 15514, … ┆ [14181, 6623, … ┆ [31421, 20937, … 22482] │\n",
      "│ 6722b5…                  ┆         ┆ 13159]          ┆ 12550]          ┆                         │\n",
      "│ b9f1c1a0fff38a3f097e0a26 ┆ 18871   ┆ [4058, 15514, … ┆ [14181, 6623, … ┆ [24331, 1395, … 26625]  │\n",
      "│ 47c12d…                  ┆         ┆ 13159]          ┆ 12550]          ┆                         │\n",
      "│ 91f84198ba89acbf93071f5c ┆ 7188    ┆ [4058, 15514, … ┆ [14181, 6623, … ┆ [15928, 23207, … 25666] │\n",
      "│ 2c75e4…                  ┆         ┆ 13159]          ┆ 12550]          ┆                         │\n",
      "│ 6ceb2674302b6a76189cb521 ┆ 3424    ┆ [4058, 15514, … ┆ [14181, 6623, … ┆ [20706, 14154, … 14920] │\n",
      "│ ac2c06…                  ┆         ┆ 13159]          ┆ 12550]          ┆                         │\n",
      "│ 6b29fd249283e29df1462f3f ┆ 19167   ┆ [4058, 15514, … ┆ [14181, 6623, … ┆ [28356, 17725, … 14600] │\n",
      "│ cc0258…                  ┆         ┆ 13159]          ┆ 12550]          ┆                         │\n",
      "└──────────────────────────┴─────────┴─────────────────┴─────────────────┴─────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train users: {train['user_id'].n_unique()}, items: {len(train_items)}\")\n",
    "print(f\"Test users: {test['user_id'].n_unique()}, items: {len(test_items)}\")\n",
    "print(f\"Cold items: {len(cold_items)}\")\n",
    "user_ids = test[\"user_id\"].unique().to_list()\n",
    "\n",
    "# Инициализируем и обучаем разные модели\n",
    "pop_model = ItemPopRecSys(train)\n",
    "pop_model.fit()\n",
    "pred_pop = pop_model.recommend(user_ids, k=10)\n",
    "\n",
    "recent_model = MostRecentItemsRecSys(train, time_col=\"date_added\")\n",
    "recent_model.fit()\n",
    "pred_recent = recent_model.recommend(user_ids, k=10)\n",
    "\n",
    "cold_model = RandomColdRecSys(list(cold_items))\n",
    "pred_cold = cold_model.recommend(user_ids, k=10)\n",
    "\n",
    "# Собираем предсказания в словарь\n",
    "models = {\n",
    "    \"Popularity\": pred_pop,\n",
    "    \"Recent\": pred_recent,\n",
    "    \"RandomCold\": pred_cold,\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"Train users: {train['user_id'].n_unique()}, items: {train['item_id'].n_unique()}\")\n",
    "train_df = show_predictions(models, train, n=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9d54da2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 7)\n",
      "┌────────────┬───────────┬──────────────┬────────────┬──────────┬──────────┬──────────┐\n",
      "│ model      ┆ Recall@10 ┆ Precision@10 ┆ HitRate@10 ┆ NDCG@10  ┆ MRR@10   ┆ Coverage │\n",
      "│ ---        ┆ ---       ┆ ---          ┆ ---        ┆ ---      ┆ ---      ┆ ---      │\n",
      "│ str        ┆ f64       ┆ f64          ┆ f64        ┆ f64      ┆ f64      ┆ f64      │\n",
      "╞════════════╪═══════════╪══════════════╪════════════╪══════════╪══════════╪══════════╡\n",
      "│ Popularity ┆ 0.032568  ┆ 0.025603     ┆ 0.163899   ┆ 0.033254 ┆ 0.053643 ┆ 0.000319 │\n",
      "│ Recent     ┆ 0.007675  ┆ 0.010843     ┆ 0.092957   ┆ 0.011604 ┆ 0.024268 ┆ 0.000319 │\n",
      "│ RandomCold ┆ 0.000249  ┆ 0.000563     ┆ 0.005295   ┆ 0.000616 ┆ 0.001635 ┆ 0.056709 │\n",
      "└────────────┴───────────┴──────────────┴────────────┴──────────┴──────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "validator = JointValidator(train, test, cold_items)\n",
    "metrics_df = val_predictions(models, test, validator, k=10, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
