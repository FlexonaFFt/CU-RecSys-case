{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9713ced",
   "metadata": {},
   "source": [
    "# Базовый солюшн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7a7cd139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict, Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "80ea04ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сразу загружу данные\n",
    "train = pl.read_parquet(\"../../data/train.pq\")\n",
    "test = pl.read_parquet(\"../../data/test.pq\")\n",
    "\n",
    "train_items = set(train[\"item_id\"].unique())\n",
    "test_items = set(test[\"item_id\"].unique())\n",
    "cold_items = test_items - train_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a5282",
   "metadata": {},
   "source": [
    "### Метрики оценивания моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2bc75e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validator(ABC):\n",
    "    def __init__(self, train: pd.DataFrame, test: pd.DataFrame, cold_items: set = None):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.cold_items = cold_items or set()\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self, predictions: Dict[int, List[int]]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        predictions: dict user_id -> list of recommended item_ids\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def recall_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        return len(set(y_true) & set(y_pred[:k])) / len(set(y_true)) if y_true else 0.0\n",
    "\n",
    "    def precision_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        return len(set(y_true) & set(y_pred[:k])) / k if y_true else 0.0\n",
    "\n",
    "    def hitrate_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        return 1.0 if len(set(y_true) & set(y_pred[:k])) > 0 else 0.0\n",
    "\n",
    "    def ndcg_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        dcg = 0.0\n",
    "        for i, item in enumerate(y_pred[:k]):\n",
    "            if item in y_true:\n",
    "                dcg += 1 / np.log2(i + 2)\n",
    "        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(y_true), k)))\n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    def mrr_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        for i, item in enumerate(y_pred[:k]):\n",
    "            if item in y_true:\n",
    "                return 1 / (i + 1)\n",
    "        return 0.0\n",
    "\n",
    "    def coverage(self, predictions: Dict[int, List[int]]) -> float:\n",
    "        all_pred_items = set(item for recs in predictions.values() for item in recs)\n",
    "        all_train_items = set(self.train[\"item_id\"].unique())\n",
    "        return len(all_pred_items) / len(all_train_items)\n",
    "\n",
    "    @staticmethod\n",
    "    def print_metrics(metrics: Dict[str, float]):\n",
    "        print(\"\\n=== Evaluation Results ===\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key:<15}: {value:.4f}\")\n",
    "        print(\"==========================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01662f85",
   "metadata": {},
   "source": [
    "Стоит разделить валидацию на две версии, для сравнения. Моя гипотеза заключается в том, что совместная валидация warm и cold может быть не совсем честной. Например, если в тесте 90% warm и 10% cold, то Recall@10 в среднем будет определяться warm-айтемами. Модель может полностью «забыть» про cold items, но в отчёте всё равно будут хорошие цифры. Это вводит в заблуждение: кажется, что модель универсальная, хотя на самом деле cold-start не решён."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7d849",
   "metadata": {},
   "source": [
    "#### Блок совместной валидации (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "64a70b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointValidator(Validator):\n",
    "    def __init__(self, train: pl.DataFrame, test: pl.DataFrame, cold_items: set = None):\n",
    "        super().__init__(train, test, cold_items)\n",
    "        self.user2items = (\n",
    "            test.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_dict(as_series=False)\n",
    "        )\n",
    "        self.user2items = dict(zip(self.user2items[\"user_id\"], self.user2items[\"item_id\"]))\n",
    "\n",
    "    def evaluate(self, predictions: Dict[int, List[int]]) -> Dict[str, float]:\n",
    "        recalls, precisions, hits, ndcgs, mrrs = [], [], [], [], []\n",
    "        for user_id, y_pred in predictions.items():\n",
    "            y_true = self.user2items.get(user_id, [])\n",
    "            recalls.append(self.recall_at_k(y_true, y_pred))\n",
    "            precisions.append(self.precision_at_k(y_true, y_pred))\n",
    "            hits.append(self.hitrate_at_k(y_true, y_pred))\n",
    "            ndcgs.append(self.ndcg_at_k(y_true, y_pred))\n",
    "            mrrs.append(self.mrr_at_k(y_true, y_pred))\n",
    "        results = {\n",
    "            \"Recall@10\": np.mean(recalls),\n",
    "            \"Precision@10\": np.mean(precisions),\n",
    "            \"HitRate@10\": np.mean(hits),\n",
    "            \"NDCG@10\": np.mean(ndcgs),\n",
    "            \"MRR@10\": np.mean(mrrs),\n",
    "            \"Coverage\": self.coverage(predictions),\n",
    "        }\n",
    "        self.print_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fbe954",
   "metadata": {},
   "source": [
    "#### Разделенная валидация (cold vs warm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fceeeaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitValidator(Validator):\n",
    "    def __init__(self, train: pl.DataFrame, test: pl.DataFrame, cold_items: set = None):\n",
    "        super().__init__(train, test, cold_items)\n",
    "        self.user2items = (\n",
    "            test.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_dict(as_series=False)\n",
    "        )\n",
    "        self.user2items = dict(zip(self.user2items[\"user_id\"], self.user2items[\"item_id\"]))\n",
    "\n",
    "    def evaluate(self, predictions: Dict[int, List[int]]) -> Dict[str, float]:\n",
    "        results = {}\n",
    "        for subset in [\"cold\", \"warm\"]:\n",
    "            recalls, precisions, hits, ndcgs, mrrs = [], [], [], [], []\n",
    "            for user_id, y_pred in predictions.items():\n",
    "                y_true = self.user2items.get(user_id, [])\n",
    "                if not y_true:\n",
    "                    continue\n",
    "                if subset == \"cold\":\n",
    "                    y_true = [i for i in y_true if i in self.cold_items]\n",
    "                elif subset == \"warm\":\n",
    "                    y_true = [i for i in y_true if i not in self.cold_items]\n",
    "                if not y_true:\n",
    "                    continue\n",
    "                recalls.append(self.recall_at_k(y_true, y_pred))\n",
    "                precisions.append(self.precision_at_k(y_true, y_pred))\n",
    "                hits.append(self.hitrate_at_k(y_true, y_pred))\n",
    "                ndcgs.append(self.ndcg_at_k(y_true, y_pred))\n",
    "                mrrs.append(self.mrr_at_k(y_true, y_pred))\n",
    "            results[f\"Recall@10_{subset}\"] = np.mean(recalls) if recalls else 0.0\n",
    "            results[f\"Precision@10_{subset}\"] = np.mean(precisions) if precisions else 0.0\n",
    "            results[f\"HitRate@10_{subset}\"] = np.mean(hits) if hits else 0.0\n",
    "            results[f\"NDCG@10_{subset}\"] = np.mean(ndcgs) if ndcgs else 0.0\n",
    "            results[f\"MRR@10_{subset}\"] = np.mean(mrrs) if mrrs else 0.0\n",
    "        results[\"Coverage\"] = self.coverage(predictions)\n",
    "        self.print_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b15366b",
   "metadata": {},
   "source": [
    "### Базовые методы "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8239bb1",
   "metadata": {},
   "source": [
    "##### Проблематика ItemPopRecSys и cold-start\n",
    "\n",
    "* `ItemPopRecSys` - рекомендует только книги, которые были в train.\n",
    "* Но cold items (которые есть только в test) он никогда не предложит.\n",
    "* __Это особенность__ - все чисто коллаборативные методы не умеют cold-start без дополнительных фичей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "17f65c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemPopRecSys:\n",
    "    def __init__(self, train: pd.DataFrame):\n",
    "        self.train = train\n",
    "        self.popular_items = None\n",
    "\n",
    "    def fit(self):\n",
    "        # Считаем популярность айтемов по числу взаимодействий\n",
    "        popularity = (self.train.group_by(\"item_id\").len().sort(\"len\", descending=True))\n",
    "        self.popular_items = popularity[\"item_id\"].to_list()\n",
    "\n",
    "    def recommend(self, user_ids: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        if self.popular_items is None:\n",
    "            raise ValueError(\"Model is not fitted. Call fit() before recommend().\")\n",
    "        preds = {u: self.popular_items[:k] for u in user_ids}\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cc2924",
   "metadata": {},
   "source": [
    "##### RandomColdRecSys\n",
    "\n",
    "Для каждого пользователя случайно выбирает cold items (те, которых не было в train). Такой метод я бы не стал использовть в реальном проекте. На мой взгляд это крайне \"дешёвый\" прием. Как раз это подтвердят метрики качества, так как случайный выбор редко совпадает с настоящими предпочтениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f4c139f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomColdRecSys:\n",
    "    def __init__(self, cold_items: List[int], seed: int = 42):\n",
    "        self.cold_items = cold_items\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def recommend(self, user_ids: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        return {\n",
    "            u: self.rng.choice(self.cold_items, size=min(k, len(self.cold_items)), replace=False).tolist()\n",
    "            for u in user_ids\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2919327b",
   "metadata": {},
   "source": [
    "##### MostRecentItemsRecSys\n",
    "\n",
    "Идея алгоритма заключается в том, чтобы брать самые новые книги. Можно сказать, что это базовый способ реализовать `cold start`. Если книга новая - возможно она будет интересна пользователю. Но такой метод вообще не опирается на желания и интересы пользователя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "bc71207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MostRecentItemsRecSys:\n",
    "    def __init__(self, items_metadata: pl.DataFrame, time_col: str = \"timestamp\"):\n",
    "        self.items_metadata = items_metadata\n",
    "        self.time_col = time_col\n",
    "        self.sorted_items = None\n",
    "\n",
    "    def fit(self):\n",
    "        if self.time_col not in self.items_metadata.columns:\n",
    "            raise ValueError(\n",
    "                f\"Column '{self.time_col}' not found. \"\n",
    "                f\"Available columns: {self.items_metadata.columns}\"\n",
    "            )\n",
    "        self.sorted_items = (\n",
    "            self.items_metadata\n",
    "            .sort(self.time_col, descending=True)[\"item_id\"]\n",
    "            .to_list()\n",
    "        )\n",
    "\n",
    "    def recommend(self, user_ids: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        if self.sorted_items is None:\n",
    "            raise ValueError(\"Model is not fitted. Call fit() before recommend().\")\n",
    "        return {u: self.sorted_items[:k] for u in user_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff180d52",
   "metadata": {},
   "source": [
    "##### Алгоритм K-ближайших соседей\n",
    "\n",
    "_Идея такая:_ выдавать похожим пользователям, похожие рекомендации, так как с большой вероятность они могут иметь схожие предпочтения.\n",
    "\n",
    "**Алгоритм**\n",
    "1. Находим пользователей, которые читали те же книги, что и целевой.\n",
    "2. Считаем пересечение (например, количество общих книг).\n",
    "3. Рекомендуем книги, которые читали соседи, но не читал целевой пользователь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ff1a8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserKNNRecSys:\n",
    "    def __init__(self, train: pl.DataFrame, k_neighbors: int = 50):\n",
    "        self.train = train\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.user2items = {}\n",
    "        self.user_neighbors = {}\n",
    "\n",
    "    def fit(self):\n",
    "        umap = self.train.group_by(\"user_id\").agg(pl.col(\"item_id\").unique()).to_dict(as_series=False)\n",
    "        self.user2items = {u: list(items) for u, items in zip(umap[\"user_id\"], umap[\"item_id\"])}\n",
    "\n",
    "        imap = self.train.group_by(\"item_id\").agg(pl.col(\"user_id\").unique()).to_dict(as_series=False)\n",
    "        item2users = {i: list(users) for i, users in zip(imap[\"item_id\"], imap[\"user_id\"])}\n",
    "\n",
    "        co_counts = defaultdict(Counter)\n",
    "        for users in item2users.values():\n",
    "            for u in users:\n",
    "                for v in users:\n",
    "                    if u != v:\n",
    "                        co_counts[u][v] += 1\n",
    "\n",
    "        for u, counter in co_counts.items():\n",
    "            self.user_neighbors[u] = [v for v, _ in counter.most_common(self.k_neighbors)]\n",
    "\n",
    "    def recommend(self, user_ids: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        preds = {}\n",
    "        for u in user_ids:\n",
    "            scores = Counter()\n",
    "            for neigh in self.user_neighbors.get(u, []):\n",
    "                for it in self.user2items.get(neigh, []):\n",
    "                    if it not in self.user2items.get(u, []):\n",
    "                        scores[it] += 1\n",
    "            preds[u] = [i for i, _ in scores.most_common(k)]\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd395434",
   "metadata": {},
   "source": [
    "##### Item KNN\n",
    "\n",
    "Поиск ближайших соседей для айтемов. \n",
    "\n",
    "**Алгоритм:**\n",
    "1. Для каждой книги считаем её соседей (какие книги читают те же пользователи).\n",
    "2. Для целевого пользователя берём его книги и ищем похожие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "17c48852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemKNNRecSys:\n",
    "    def __init__(self, train: pl.DataFrame, k_neighbors: int = 50):\n",
    "        self.train = train\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.user2items = {}\n",
    "        self.item_neighbors = {}\n",
    "\n",
    "    def fit(self):\n",
    "        umap = self.train.group_by(\"user_id\").agg(pl.col(\"item_id\").unique()).to_dict(as_series=False)\n",
    "        self.user2items = {u: list(items) for u, items in zip(umap[\"user_id\"], umap[\"item_id\"])}\n",
    "        co_counts = defaultdict(Counter)\n",
    "        for items in self.user2items.values():\n",
    "            for i in items:\n",
    "                for j in items:\n",
    "                    if i != j:\n",
    "                        co_counts[i][j] += 1\n",
    "\n",
    "        for i, counter in co_counts.items():\n",
    "            self.item_neighbors[i] = [j for j, _ in counter.most_common(self.k_neighbors)]\n",
    "\n",
    "    def recommend(self, user_ids: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        preds = {}\n",
    "        for u in user_ids:\n",
    "            items_u = self.user2items.get(u, [])\n",
    "            scores = Counter()\n",
    "            for it in items_u:\n",
    "                for neigh in self.item_neighbors.get(it, []):\n",
    "                    if neigh not in items_u:\n",
    "                        scores[neigh] += 1\n",
    "            preds[u] = [i for i, _ in scores.most_common(k)]\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83804efa",
   "metadata": {},
   "source": [
    "##### HybridPopColdRecSys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3bc68b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridPopColdRecSys:\n",
    "    def __init__(self, train: pl.DataFrame, cold_items: List[int]):\n",
    "        self.train = train\n",
    "        self.cold_items = cold_items\n",
    "        self.popular_items = None\n",
    "\n",
    "    def fit(self):\n",
    "        popularity = (\n",
    "            self.train.group_by(\"item_id\").len().sort(\"len\", descending=True)\n",
    "        )\n",
    "        self.popular_items = popularity[\"item_id\"].to_list()\n",
    "\n",
    "    def recommend(self, user_ids: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        preds = {}\n",
    "        for u in user_ids:\n",
    "            recs = self.popular_items[: k // 2]\n",
    "            if self.cold_items:\n",
    "                recs += list(np.random.choice(self.cold_items, size=min(k - len(recs), len(self.cold_items)), replace=False))\n",
    "            preds[u] = recs[:k]\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb8b02",
   "metadata": {},
   "source": [
    "#### Вспомогательные функции для удобного представления результатов\n",
    "\n",
    "Необходимы для составления табличек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ce3f56bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shorten_list(lst, max_len=10):\n",
    "    \"\"\"Обрезает длинные списки для красивого вывода\"\"\"\n",
    "    if lst is None:\n",
    "        return []\n",
    "    return lst[:max_len] if len(lst) > max_len else lst\n",
    "\n",
    "def show_predictions(models: dict, data: pl.DataFrame, n=5, verbose=True, is_val=False):\n",
    "    df = data.sample(n).select([\"user_id\", \"item_id\"])\n",
    "    if is_val:\n",
    "        df = df.rename({\"item_id\": \"true_items\"})\n",
    "\n",
    "    # добавляем предсказания\n",
    "    for name, preds in models.items():\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"user_id\").map_elements(\n",
    "                lambda u: _shorten_list(preds.get(u, [])), \n",
    "                return_dtype=pl.List(pl.Int64)\n",
    "            ).alias(name)\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        print(df.shape)\n",
    "        print(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def val_predictions(models: dict, val: pl.DataFrame, validator: Validator, k: int = 10, verbose: bool = True):\n",
    "    results = []\n",
    "    user2items = (\n",
    "        val.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_dict(as_series=False)\n",
    "    )\n",
    "    user2items = dict(zip(user2items[\"user_id\"], user2items[\"item_id\"]))\n",
    "\n",
    "    for model_name, preds in models.items():\n",
    "        recalls, precisions, hits, ndcgs, mrrs = [], [], [], [], []\n",
    "        for u, y_true in user2items.items():\n",
    "            y_pred = preds.get(u, [])\n",
    "            recalls.append(validator.recall_at_k(y_true, y_pred, k))\n",
    "            precisions.append(validator.precision_at_k(y_true, y_pred, k))\n",
    "            hits.append(validator.hitrate_at_k(y_true, y_pred, k))\n",
    "            ndcgs.append(validator.ndcg_at_k(y_true, y_pred, k))\n",
    "            mrrs.append(validator.mrr_at_k(y_true, y_pred, k))\n",
    "        metrics = {\n",
    "            \"model\": model_name,\n",
    "            \"Recall@10\": np.mean(recalls),\n",
    "            \"Precision@10\": np.mean(precisions),\n",
    "            \"HitRate@10\": np.mean(hits),\n",
    "            \"NDCG@10\": np.mean(ndcgs),\n",
    "            \"MRR@10\": np.mean(mrrs),\n",
    "            \"Coverage\": validator.coverage(preds),\n",
    "        }\n",
    "        results.append(metrics)\n",
    "\n",
    "    df = pl.DataFrame(results)\n",
    "    if verbose:\n",
    "        print(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffc5ce4",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5e1a315e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train users: 349719, items: 31300\n",
      "Test users: 185828, items: 27367\n",
      "Cold items: 1775\n",
      "\n",
      "Рекомендации моделей:\n",
      "(5, 6)\n",
      "shape: (5, 6)\n",
      "┌─────────────────┬─────────┬─────────────────┬─────────────────┬─────────────────┬────────────────┐\n",
      "│ user_id         ┆ item_id ┆ Popularity      ┆ Recent          ┆ RandomCold      ┆ HybridMethod   │\n",
      "│ ---             ┆ ---     ┆ ---             ┆ ---             ┆ ---             ┆ ---            │\n",
      "│ str             ┆ i64     ┆ list[i64]       ┆ list[i64]       ┆ list[i64]       ┆ list[i64]      │\n",
      "╞═════════════════╪═════════╪═════════════════╪═════════════════╪═════════════════╪════════════════╡\n",
      "│ 3e4f2d2423b7c7d ┆ 8765    ┆ [4058, 15514, … ┆ [14181, 6623, … ┆ [20271, 14062,  ┆ [4058, 15514,  │\n",
      "│ da532dcc2391d01 ┆         ┆ 13159]          ┆ 12550]          ┆ … 21366]        ┆ … 27428]       │\n",
      "│ …               ┆         ┆                 ┆                 ┆                 ┆                │\n",
      "│ 28e3a1d990de7f8 ┆ 26228   ┆ []              ┆ []              ┆ []              ┆ []             │\n",
      "│ 68607ddf25f432e ┆         ┆                 ┆                 ┆                 ┆                │\n",
      "│ …               ┆         ┆                 ┆                 ┆                 ┆                │\n",
      "│ f375cfe5a2278e2 ┆ 27588   ┆ [4058, 15514, … ┆ [14181, 6623, … ┆ [17090, 20830,  ┆ [4058, 15514,  │\n",
      "│ 841ac7632551f1d ┆         ┆ 13159]          ┆ 12550]          ┆ … 31636]        ┆ … 7174]        │\n",
      "│ …               ┆         ┆                 ┆                 ┆                 ┆                │\n",
      "│ a122e9b9a9654d1 ┆ 1003    ┆ [4058, 15514, … ┆ [14181, 6623, … ┆ [32209, 7061, … ┆ [4058, 15514,  │\n",
      "│ 373a8ea52a43b53 ┆         ┆ 13159]          ┆ 12550]          ┆ 20455]          ┆ … 7496]        │\n",
      "│ …               ┆         ┆                 ┆                 ┆                 ┆                │\n",
      "│ ed9e04843a5e2bc ┆ 14196   ┆ [4058, 15514, … ┆ [14181, 6623, … ┆ [6721, 11956, … ┆ [4058, 15514,  │\n",
      "│ 669083131f4cc05 ┆         ┆ 13159]          ┆ 12550]          ┆ 18228]          ┆ … 7275]        │\n",
      "│ …               ┆         ┆                 ┆                 ┆                 ┆                │\n",
      "└─────────────────┴─────────┴─────────────────┴─────────────────┴─────────────────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train users: {train['user_id'].n_unique()}, items: {len(train_items)}\")\n",
    "print(f\"Test users: {test['user_id'].n_unique()}, items: {len(test_items)}\")\n",
    "print(f\"Cold items: {len(cold_items)}\")\n",
    "user_ids = test[\"user_id\"].unique().to_list()\n",
    "\n",
    "# Инициализируем и обучаем разные модели\n",
    "pop_model = ItemPopRecSys(train)\n",
    "pop_model.fit()\n",
    "pred_pop = pop_model.recommend(user_ids, k=10)\n",
    "\n",
    "recent_model = MostRecentItemsRecSys(train, time_col=\"date_added\")\n",
    "recent_model.fit()\n",
    "pred_recent = recent_model.recommend(user_ids, k=10)\n",
    "\n",
    "cold_model = RandomColdRecSys(list(cold_items))\n",
    "pred_cold = cold_model.recommend(user_ids, k=10)\n",
    "\n",
    "hybrid_model = HybridPopColdRecSys(train, list(cold_items))\n",
    "hybrid_model.fit()\n",
    "pred_hybrid = hybrid_model.recommend(user_ids, k=10)\n",
    "\n",
    "''' item_knn и user_knn просчитываются слишком долго\n",
    "\n",
    "item_knn = ItemKNNRecSys(train)\n",
    "item_knn.fit()\n",
    "pred_item_knn = item_knn.recommend(user_ids, k=10)\n",
    "\n",
    "user_knn = UserKNNRecSys(train)\n",
    "user_knn.fit()\n",
    "pred_user_knn = user_knn.recommend(user_ids, k=10)'''\n",
    "\n",
    "# Собираем предсказания в словарь\n",
    "models = {\n",
    "    \"Popularity\": pred_pop,\n",
    "    \"Recent\": pred_recent,\n",
    "    \"RandomCold\": pred_cold,\n",
    "    \"HybridMethod\": pred_hybrid,\n",
    "}\n",
    "\n",
    "\n",
    "print(\"\\nРекомендации моделей:\")\n",
    "train_df = show_predictions(models, train, n=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9d54da2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Статистика по метрикам для каждой модели:\n",
      "shape: (4, 7)\n",
      "┌──────────────┬───────────┬──────────────┬────────────┬──────────┬──────────┬──────────┐\n",
      "│ model        ┆ Recall@10 ┆ Precision@10 ┆ HitRate@10 ┆ NDCG@10  ┆ MRR@10   ┆ Coverage │\n",
      "│ ---          ┆ ---       ┆ ---          ┆ ---        ┆ ---      ┆ ---      ┆ ---      │\n",
      "│ str          ┆ f64       ┆ f64          ┆ f64        ┆ f64      ┆ f64      ┆ f64      │\n",
      "╞══════════════╪═══════════╪══════════════╪════════════╪══════════╪══════════╪══════════╡\n",
      "│ Popularity   ┆ 0.032568  ┆ 0.025603     ┆ 0.163899   ┆ 0.033254 ┆ 0.053643 ┆ 0.000319 │\n",
      "│ Recent       ┆ 0.007675  ┆ 0.010843     ┆ 0.092957   ┆ 0.011604 ┆ 0.024268 ┆ 0.000319 │\n",
      "│ RandomCold   ┆ 0.000248  ┆ 0.000561     ┆ 0.005263   ┆ 0.000595 ┆ 0.001515 ┆ 0.056709 │\n",
      "│ HybridMethod ┆ 0.018637  ┆ 0.01358      ┆ 0.109542   ┆ 0.022265 ┆ 0.046009 ┆ 0.056869 │\n",
      "└──────────────┴───────────┴──────────────┴────────────┴──────────┴──────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nСтатистика по метрикам для каждой модели:\")\n",
    "validator = JointValidator(train, test, cold_items)\n",
    "metrics_df = val_predictions(models, test, validator, k=10, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
