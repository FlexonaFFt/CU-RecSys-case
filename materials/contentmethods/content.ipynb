{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25497418",
   "metadata": {},
   "source": [
    "# Контентные методы "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8688bfe8",
   "metadata": {},
   "source": [
    "### Базовые операции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2afc3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import logging\n",
    "import joblib\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict, Counter \n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "os.makedirs('../models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aecb716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сразу загружу данные\n",
    "train = pl.read_parquet(\"../../data/train.pq\")\n",
    "test = pl.read_parquet(\"../../data/test.pq\")\n",
    "books = pl.read_parquet(\"../../data/books.pq\")\n",
    "\n",
    "train_items = set(train[\"item_id\"].unique())\n",
    "test_items = set(test[\"item_id\"].unique())\n",
    "cold_items = test_items - train_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc86f568",
   "metadata": {},
   "source": [
    "#### Метрики оценивания моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b46c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validator(ABC):\n",
    "    def __init__(self, train: pd.DataFrame, test: pd.DataFrame, cold_items: set = None):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.cold_items = cold_items or set()\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self, predictions: Dict[int, List[int]]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        predictions: dict user_id -> list of recommended item_ids\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def recall_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        return len(set(y_true) & set(y_pred[:k])) / len(set(y_true)) if y_true else 0.0\n",
    "\n",
    "    def precision_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        return len(set(y_true) & set(y_pred[:k])) / k if y_true else 0.0\n",
    "\n",
    "    def hitrate_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        return 1.0 if len(set(y_true) & set(y_pred[:k])) > 0 else 0.0\n",
    "\n",
    "    def ndcg_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        dcg = 0.0\n",
    "        for i, item in enumerate(y_pred[:k]):\n",
    "            if item in y_true:\n",
    "                dcg += 1 / np.log2(i + 2)\n",
    "        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(y_true), k)))\n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    def mrr_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        for i, item in enumerate(y_pred[:k]):\n",
    "            if item in y_true:\n",
    "                return 1 / (i + 1)\n",
    "        return 0.0\n",
    "\n",
    "    def coverage(self, predictions: Dict[int, List[int]]) -> float:\n",
    "        all_pred_items = set(item for recs in predictions.values() for item in recs)\n",
    "        all_train_items = set(self.train[\"item_id\"].unique())\n",
    "        return len(all_pred_items) / len(all_train_items)\n",
    "\n",
    "    @staticmethod\n",
    "    def print_metrics(metrics: Dict[str, float]):\n",
    "        print(\"\\n=== Evaluation Results ===\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key:<15}: {value:.4f}\")\n",
    "        print(\"==========================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e5b41",
   "metadata": {},
   "source": [
    "Стоит разделить валидацию на две версии, для сравнения. Моя гипотеза заключается в том, что совместная валидация warm и cold может быть не совсем честной. Например, если в тесте 90% warm и 10% cold, то Recall@10 в среднем будет определяться warm-айтемами. Модель может полностью «забыть» про cold items, но в отчёте всё равно будут хорошие цифры. Это вводит в заблуждение: кажется, что модель универсальная, хотя на самом деле cold-start не решён."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada31206",
   "metadata": {},
   "source": [
    "#### Блок совместной валидации (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8fd76c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointValidator(Validator):\n",
    "    def __init__(self, train: pl.DataFrame, test: pl.DataFrame, cold_items: set = None):\n",
    "        super().__init__(train, test, cold_items)\n",
    "        self.user2items = (\n",
    "            test.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_dict(as_series=False)\n",
    "        )\n",
    "        self.user2items = dict(zip(self.user2items[\"user_id\"], self.user2items[\"item_id\"]))\n",
    "\n",
    "    def evaluate(self, predictions: Dict[int, List[int]]) -> Dict[str, float]:\n",
    "        recalls, precisions, hits, ndcgs, mrrs = [], [], [], [], []\n",
    "        for user_id, y_pred in predictions.items():\n",
    "            y_true = self.user2items.get(user_id, [])\n",
    "            recalls.append(self.recall_at_k(y_true, y_pred))\n",
    "            precisions.append(self.precision_at_k(y_true, y_pred))\n",
    "            hits.append(self.hitrate_at_k(y_true, y_pred))\n",
    "            ndcgs.append(self.ndcg_at_k(y_true, y_pred))\n",
    "            mrrs.append(self.mrr_at_k(y_true, y_pred))\n",
    "        results = {\n",
    "            \"Recall@10\": np.mean(recalls),\n",
    "            \"Precision@10\": np.mean(precisions),\n",
    "            \"HitRate@10\": np.mean(hits),\n",
    "            \"NDCG@10\": np.mean(ndcgs),\n",
    "            \"MRR@10\": np.mean(mrrs),\n",
    "            \"Coverage\": self.coverage(predictions),\n",
    "        }\n",
    "        self.print_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c9d8c",
   "metadata": {},
   "source": [
    "#### Разделенная валидация (cold vs warm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4fcf8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitValidator(Validator):\n",
    "    def __init__(self, train: pl.DataFrame, test: pl.DataFrame, cold_items: set = None):\n",
    "        super().__init__(train, test, cold_items)\n",
    "        self.user2items = (\n",
    "            test.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_dict(as_series=False)\n",
    "        )\n",
    "        self.user2items = dict(zip(self.user2items[\"user_id\"], self.user2items[\"item_id\"]))\n",
    "\n",
    "    def evaluate(self, predictions: Dict[int, List[int]]) -> Dict[str, float]:\n",
    "        results = {}\n",
    "        for subset in [\"cold\", \"warm\"]:\n",
    "            recalls, precisions, hits, ndcgs, mrrs = [], [], [], [], []\n",
    "            for user_id, y_pred in predictions.items():\n",
    "                y_true = self.user2items.get(user_id, [])\n",
    "                if not y_true:\n",
    "                    continue\n",
    "                if subset == \"cold\":\n",
    "                    y_true = [i for i in y_true if i in self.cold_items]\n",
    "                elif subset == \"warm\":\n",
    "                    y_true = [i for i in y_true if i not in self.cold_items]\n",
    "                if not y_true:\n",
    "                    continue\n",
    "                recalls.append(self.recall_at_k(y_true, y_pred))\n",
    "                precisions.append(self.precision_at_k(y_true, y_pred))\n",
    "                hits.append(self.hitrate_at_k(y_true, y_pred))\n",
    "                ndcgs.append(self.ndcg_at_k(y_true, y_pred))\n",
    "                mrrs.append(self.mrr_at_k(y_true, y_pred))\n",
    "            results[f\"Recall@10_{subset}\"] = np.mean(recalls) if recalls else 0.0\n",
    "            results[f\"Precision@10_{subset}\"] = np.mean(precisions) if precisions else 0.0\n",
    "            results[f\"HitRate@10_{subset}\"] = np.mean(hits) if hits else 0.0\n",
    "            results[f\"NDCG@10_{subset}\"] = np.mean(ndcgs) if ndcgs else 0.0\n",
    "            results[f\"MRR@10_{subset}\"] = np.mean(mrrs) if mrrs else 0.0\n",
    "        results[\"Coverage\"] = self.coverage(predictions)\n",
    "        self.print_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5488f3",
   "metadata": {},
   "source": [
    "#### Вспомогательные функции для удобного представления результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d934e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shorten_list(lst, max_len=10):\n",
    "    \"\"\"Обрезает длинные списки для красивого вывода\"\"\"\n",
    "    if lst is None:\n",
    "        return []\n",
    "    return lst[:max_len] if len(lst) > max_len else lst\n",
    "\n",
    "def show_predictions(models: dict, data: pl.DataFrame, n=5, verbose=True, is_val=False):\n",
    "    df = data.sample(n).select([\"user_id\", \"item_id\"])\n",
    "    if is_val:\n",
    "        df = df.rename({\"item_id\": \"true_items\"})\n",
    "\n",
    "    # добавляем предсказания\n",
    "    for name, preds in models.items():\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"user_id\").map_elements(\n",
    "                lambda u: _shorten_list(preds.get(u, [])), \n",
    "                return_dtype=pl.List(pl.Int64)\n",
    "            ).alias(name)\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        print(df.shape)\n",
    "        print(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def val_predictions(models: dict, val: pl.DataFrame, validator: Validator, k: int = 10, verbose: bool = True):\n",
    "    results = []\n",
    "    user2items = (\n",
    "        val.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_dict(as_series=False)\n",
    "    )\n",
    "    user2items = dict(zip(user2items[\"user_id\"], user2items[\"item_id\"]))\n",
    "\n",
    "    for model_name, preds in models.items():\n",
    "        recalls, precisions, hits, ndcgs, mrrs = [], [], [], [], []\n",
    "        for u, y_true in user2items.items():\n",
    "            y_pred = preds.get(u, [])\n",
    "            recalls.append(validator.recall_at_k(y_true, y_pred, k))\n",
    "            precisions.append(validator.precision_at_k(y_true, y_pred, k))\n",
    "            hits.append(validator.hitrate_at_k(y_true, y_pred, k))\n",
    "            ndcgs.append(validator.ndcg_at_k(y_true, y_pred, k))\n",
    "            mrrs.append(validator.mrr_at_k(y_true, y_pred, k))\n",
    "        metrics = {\n",
    "            \"model\": model_name,\n",
    "            \"Recall@10\": np.mean(recalls),\n",
    "            \"Precision@10\": np.mean(precisions),\n",
    "            \"HitRate@10\": np.mean(hits),\n",
    "            \"NDCG@10\": np.mean(ndcgs),\n",
    "            \"MRR@10\": np.mean(mrrs),\n",
    "            \"Coverage\": validator.coverage(preds),\n",
    "        }\n",
    "        results.append(metrics)\n",
    "\n",
    "    df = pl.DataFrame(results)\n",
    "    if verbose:\n",
    "        print(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624119e0",
   "metadata": {},
   "source": [
    "#### Функция векторизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorizer:\n",
    "    def __init__(self, max_features: int = 200, stop_words: str = 'english'):  # Уменьшено до 200\n",
    "        self.vectorizer = TfidfVectorizer(max_features=max_features, stop_words=stop_words)\n",
    "        self.item_vectors = None\n",
    "        self.item_ids = None\n",
    "\n",
    "    def fit(self, metadata: pl.DataFrame):\n",
    "        logger.info(\"Начало TF-IDF векторизации...\")\n",
    "        logger.info(f\"Использование памяти: {psutil.virtual_memory().percent}%\")\n",
    "        \n",
    "        # Подготовка текстовых данных\n",
    "        metadata = metadata.with_columns(\n",
    "            pl.col(\"description\").fill_null(\"\") + \" \" + \n",
    "            pl.col(\"tags\").list.join(\" \").fill_null(\"\")\n",
    "        )\n",
    "        corpus = metadata[\"description\"].to_list()\n",
    "        self.item_ids = metadata[\"item_id\"].to_list()\n",
    "\n",
    "        # Векторизация\n",
    "        self.item_vectors = self.vectorizer.fit_transform(corpus)\n",
    "        logger.info(f\"TF-IDF векторизация завершена. Размер матрицы: {self.item_vectors.shape}\")\n",
    "        logger.info(f\"Использование памяти после векторизации: {psutil.virtual_memory().percent}%\")\n",
    "\n",
    "    def save(self, path: str):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "            joblib.dump({\n",
    "                'vectorizer': self.vectorizer,\n",
    "                'item_vectors': self.item_vectors,\n",
    "                'item_ids': self.item_ids\n",
    "            }, path)\n",
    "            logger.info(f\"Векторизатор сохранен в {path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка при сохранении векторизатора: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load(self, path: str):\n",
    "        try:\n",
    "            data = joblib.load(path)\n",
    "            self.vectorizer = data['vectorizer']\n",
    "            self.item_vectors = data['item_vectors']\n",
    "            self.item_ids = data['item_ids']\n",
    "            logger.info(f\"Векторизатор загружен из {path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка при загрузке векторизатора: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289159ad",
   "metadata": {},
   "source": [
    "#### Базовый класс для загрузки моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc2c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender(ABC):\n",
    "    def __init__(self, model_path: str = None):\n",
    "        self.model = None\n",
    "        self.model_path = model_path\n",
    "        if model_path and os.path.exists(model_path):\n",
    "            self.load_model()\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, train: pl.DataFrame, vectorizer: TextVectorizer = None):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, user_ids: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        pass\n",
    "\n",
    "    def save_model(self):\n",
    "        if self.model_path:\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(self.model_path), exist_ok=True)\n",
    "                joblib.dump(self.model, self.model_path)\n",
    "                logger.info(f\"Модель сохранена в {self.model_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Ошибка при сохранении модели в {self.model_path}: {e}\")\n",
    "                raise\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model_path and os.path.exists(self.model_path):\n",
    "            try:\n",
    "                self.model = joblib.load(self.model_path)\n",
    "                logger.info(f\"Модель загружена из {self.model_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Ошибка при загрузке модели из {self.model_path}: {e}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bce9b2",
   "metadata": {},
   "source": [
    "### Использование контентных методов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c501b",
   "metadata": {},
   "source": [
    "#### TF-IDF на дополнительных данных\n",
    "\n",
    "Будем представлять данные в векторном формате (description, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b625394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFRecommender(Recommender):\n",
    "    def __init__(self, model_path: str = \"../models/tfidf/tfidf_recommender.pkl\", force_retrain: bool = False):\n",
    "        super().__init__(model_path)\n",
    "        self.vectorizer = None\n",
    "        self.item_vectors = None\n",
    "        self.user_profiles = None\n",
    "        self.item_ids = None\n",
    "        self.force_retrain = force_retrain\n",
    "\n",
    "    def fit(self, train: pl.DataFrame, vectorizer: TextVectorizer = None):\n",
    "        if not isinstance(vectorizer, TextVectorizer):\n",
    "            logger.error(\"Аргумент 'vectorizer' должен быть экземпляром TextVectorizer\")\n",
    "            raise TypeError(\"Аргумент 'vectorizer' должен быть экземпляром TextVectorizer\")\n",
    "        if self.model_path and os.path.exists(self.model_path) and not self.force_retrain:\n",
    "            self.load_model()\n",
    "            return\n",
    "\n",
    "        logger.info(\"Начало обучения TFIDFRecommender...\")\n",
    "        logger.info(f\"Использование памяти: {psutil.virtual_memory().percent}%\")\n",
    "\n",
    "        # Используем общий векторизатор\n",
    "        self.vectorizer = vectorizer.vectorizer\n",
    "        self.item_vectors = vectorizer.item_vectors\n",
    "        self.item_ids = vectorizer.item_ids\n",
    "\n",
    "        # Создание профилей пользователей\n",
    "        self.user_profiles = {}\n",
    "        user_items = train.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_pandas()\n",
    "        item_id_to_idx = {item_id: idx for idx, item_id in enumerate(self.item_ids)}\n",
    "        \n",
    "        for _, row in tqdm(user_items.iterrows(), total=len(user_items), desc=\"Создание профилей пользователей\"):\n",
    "            user_id = row[\"user_id\"]\n",
    "            items = row[\"item_id\"]\n",
    "            valid_items = [item for item in items if item in item_id_to_idx]\n",
    "            if valid_items:\n",
    "                item_indices = [item_id_to_idx[item] for item in valid_items]\n",
    "                user_vector = np.mean(self.item_vectors[item_indices].toarray(), axis=0)\n",
    "                self.user_profiles[user_id] = user_vector\n",
    "\n",
    "        logger.info(f\"Создано профилей пользователей: {len(self.user_profiles)}\")\n",
    "        logger.info(f\"Использование памяти после создания профилей: {psutil.virtual_memory().percent}%\")\n",
    "        self.save_model()\n",
    "\n",
    "    def predict(self, user_ids: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        predictions = {}\n",
    "        for user_id in tqdm(user_ids, desc=\"Predicting with TF-IDF\"):\n",
    "            if user_id in self.user_profiles:\n",
    "                user_vector = self.user_profiles[user_id].reshape(1, -1)\n",
    "                similarities = cosine_similarity(user_vector, self.item_vectors)[0]\n",
    "                top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "                predictions[user_id] = [self.item_ids[idx] for idx in top_k_indices]\n",
    "            else:\n",
    "                # Для новых пользователей: рекомендуем популярные книги\n",
    "                popular_items = train.group_by(\"item_id\").agg(pl.len()).sort(\"len\", descending=True).head(k)[\"item_id\"].to_list()\n",
    "                predictions[user_id] = popular_items\n",
    "        return predictions\n",
    "\n",
    "    def save_model(self):\n",
    "        if self.model_path:\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(self.model_path), exist_ok=True)\n",
    "                joblib.dump({\n",
    "                    'vectorizer': self.vectorizer,\n",
    "                    'item_vectors': self.item_vectors,\n",
    "                    'user_profiles': self.user_profiles,\n",
    "                    'item_ids': self.item_ids\n",
    "                }, self.model_path)\n",
    "                logger.info(f\"Модель сохранена в {self.model_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Ошибка при сохранении TFIDFRecommender: {e}\")\n",
    "                raise\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model_path and os.path.exists(self.model_path):\n",
    "            try:\n",
    "                data = joblib.load(self.model_path)\n",
    "                self.vectorizer = data['vectorizer']\n",
    "                self.item_vectors = data['item_vectors']\n",
    "                self.user_profiles = data['user_profiles']\n",
    "                self.item_ids = data['item_ids']\n",
    "                logger.info(f\"Модель TFIDFRecommender загружена из {self.model_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Ошибка при загрузке TFIDFRecommender: {e}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969c198",
   "metadata": {},
   "source": [
    "#### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a631dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatBoostRecommender(Recommender):\n",
    "    def __init__(self, model_path: str = \"../models/catboost/catboost_recommender.pkl\", force_retrain: bool = False):\n",
    "        super().__init__(model_path)\n",
    "        self.vectorizer = None\n",
    "        self.item_vectors = None\n",
    "        self.item_ids = None\n",
    "        self.force_retrain = force_retrain\n",
    "\n",
    "    def fit(self, train: pl.DataFrame, vectorizer: TextVectorizer = None):\n",
    "        if not isinstance(vectorizer, TextVectorizer):\n",
    "            logger.error(\"Аргумент 'vectorizer' должен быть экземпляром TextVectorizer\")\n",
    "            raise TypeError(\"Аргумент 'vectorizer' должен быть экземпляром TextVectorizer\")\n",
    "        if self.model_path and os.path.exists(self.model_path) and not self.force_retrain:\n",
    "            self.load_model()\n",
    "            return\n",
    "\n",
    "        logger.info(\"Начало обучения CatBoostRecommender...\")\n",
    "        logger.info(f\"Использование памяти: {psutil.virtual_memory().percent}%\")\n",
    "\n",
    "        # Используем общий векторизатор\n",
    "        self.vectorizer = vectorizer.vectorizer\n",
    "        self.item_vectors = vectorizer.item_vectors\n",
    "        self.item_ids = vectorizer.item_ids\n",
    "\n",
    "        # Путь для сохранения промежуточных данных\n",
    "        data_path = \"../models/catboost/catboost_data.pkl\"\n",
    "        if os.path.exists(data_path) and not self.force_retrain:\n",
    "            logger.info(\"Загрузка сохраненных данных для CatBoost...\")\n",
    "            data = joblib.load(data_path)\n",
    "            X, y = data['X'], data['y']\n",
    "        else:\n",
    "            # Подготовка данных для CatBoost с пакетной записью на диск\n",
    "            X, y = [], []\n",
    "            user_items = train.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_pandas()\n",
    "            item_id_to_idx = {item_id: idx for idx, item_id in enumerate(self.item_ids)}\n",
    "            batch_size = 1000  # Обрабатываем по 1000 пользователей за раз\n",
    "            batch_count = 0\n",
    "\n",
    "            for i in tqdm(range(0, len(user_items), batch_size), desc=\"Подготовка данных для CatBoost\"):\n",
    "                batch = user_items[i:i+batch_size]\n",
    "                batch_X, batch_y = [], []\n",
    "                for _, row in batch.iterrows():\n",
    "                    user_id = row[\"user_id\"]\n",
    "                    pos_items = set(row[\"item_id\"])\n",
    "                    neg_items = list(set(self.item_ids) - pos_items)[:min(len(pos_items), 5)]  # Уменьшено до 5\n",
    "\n",
    "                    # Используем разреженные матрицы для экономии памяти\n",
    "                    valid_pos_items = [item for item in pos_items if item in item_id_to_idx]\n",
    "                    user_vector = np.mean(\n",
    "                        [self.item_vectors[item_id_to_idx[item]].toarray()[0] for item in valid_pos_items],\n",
    "                        axis=0\n",
    "                    ) if valid_pos_items else np.zeros(self.item_vectors.shape[1])\n",
    "\n",
    "                    for item_id in pos_items:\n",
    "                        if item_id in item_id_to_idx:\n",
    "                            item_vector = self.item_vectors[item_id_to_idx[item_id]].toarray()[0]\n",
    "                            batch_X.append(np.concatenate([user_vector, item_vector]))\n",
    "                            batch_y.append(1)\n",
    "\n",
    "                    for item_id in neg_items:\n",
    "                        if item_id in item_id_to_idx:\n",
    "                            item_vector = self.item_vectors[item_id_to_idx[item_id]].toarray()[0]\n",
    "                            batch_X.append(np.concatenate([user_vector, item_vector]))\n",
    "                            batch_y.append(0)\n",
    "\n",
    "                # Сохранение батча на диск\n",
    "                batch_path = f\"../models/catboost/catboost_data_batch_{batch_count}.pkl\"\n",
    "                try:\n",
    "                    os.makedirs(os.path.dirname(batch_path), exist_ok=True)\n",
    "                    joblib.dump({'X': batch_X, 'y': batch_y}, batch_path)\n",
    "                    logger.info(f\"Батч {batch_count} сохранен в {batch_path}\")\n",
    "                    batch_count += 1\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Ошибка при сохранении батча {batch_count}: {e}\")\n",
    "                    raise\n",
    "                # Очистка памяти\n",
    "                batch_X, batch_y = [], []\n",
    "                gc.collect()\n",
    "\n",
    "            # Объединение батчей\n",
    "            X, y = [], []\n",
    "            for i in range(batch_count):\n",
    "                batch_data = joblib.load(f\"../models/catboost/catboost_data_batch_{i}.pkl\")\n",
    "                X.extend(batch_data['X'])\n",
    "                y.extend(batch_data['y'])\n",
    "                os.remove(f\"../models/catboost/catboost_data_batch_{i}.pkl\")  # Удаление временного файла\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
    "                joblib.dump({'X': X, 'y': y}, data_path)\n",
    "                logger.info(f\"Данные CatBoost сохранены в {data_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Ошибка при сохранении данных CatBoost: {e}\")\n",
    "                raise\n",
    "\n",
    "        logger.info(f\"Создано {len(X)} пар пользователь-книга\")\n",
    "        logger.info(f\"Использование памяти после подготовки данных: {psutil.virtual_memory().percent}%\")\n",
    "\n",
    "        # Обучение CatBoost\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        train_pool = Pool(X_train, y_train)\n",
    "        val_pool = Pool(X_val, y_val)\n",
    "\n",
    "        self.model = CatBoostClassifier(iterations=30, depth=6, learning_rate=0.1, thread_count=-1, verbose=10)\n",
    "        self.model.fit(train_pool, eval_set=val_pool)\n",
    "        logger.info(\"Обучение CatBoost завершено\")\n",
    "\n",
    "        self.save_model()\n",
    "\n",
    "    def predict(self, user_ids: List[int], k: int = 10, batch_size: int = 1000) -> Dict[int, List[int]]:\n",
    "        predictions = {}\n",
    "        item_id_to_idx = {item_id: idx for idx, item_id in enumerate(self.item_ids)}\n",
    "\n",
    "        for i in tqdm(range(0, len(user_ids), batch_size), desc=\"Predicting with CatBoost\"):\n",
    "            batch_users = user_ids[i:i+batch_size]\n",
    "            for user_id in batch_users:\n",
    "                user_items = train.filter(pl.col(\"user_id\") == user_id)[\"item_id\"].to_list()\n",
    "                user_vector = np.mean(\n",
    "                    [self.item_vectors[item_id_to_idx[item]].toarray()[0] for item in user_items if item in item_id_to_idx],\n",
    "                    axis=0\n",
    "                ) if user_items else np.zeros(self.item_vectors.shape[1])\n",
    "\n",
    "                X_pred = []\n",
    "                pred_items = []\n",
    "                for item_id in self.item_ids:\n",
    "                    item_vector = self.item_vectors[item_id_to_idx[item_id]].toarray()[0]\n",
    "                    X_pred.append(np.concatenate([user_vector, item_vector]))\n",
    "                    pred_items.append(item_id)\n",
    "\n",
    "                scores = self.model.predict_proba(X_pred)[:, 1]\n",
    "                top_k_indices = np.argsort(scores)[::-1][:k]\n",
    "                predictions[user_id] = [pred_items[idx] for idx in top_k_indices]\n",
    "            gc.collect()  # Очистка памяти после батча\n",
    "        return predictions\n",
    "\n",
    "    def save_model(self):\n",
    "        if self.model_path:\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(self.model_path), exist_ok=True)\n",
    "                joblib.dump({\n",
    "                    'model': self.model,\n",
    "                    'vectorizer': self.vectorizer,\n",
    "                    'item_vectors': self.item_vectors,\n",
    "                    'item_ids': self.item_ids\n",
    "                }, self.model_path)\n",
    "                logger.info(f\"Модель сохранена в {self.model_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Ошибка при сохранении CatBoostRecommender: {e}\")\n",
    "                raise\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model_path and os.path.exists(self.model_path):\n",
    "            try:\n",
    "                data = joblib.load(self.model_path)\n",
    "                self.model = data['model']\n",
    "                self.vectorizer = data['vectorizer']\n",
    "                self.item_vectors = data['item_vectors']\n",
    "                self.item_ids = data['item_ids']\n",
    "                logger.info(f\"Модель CatBoostRecommender загружена из {self.model_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Ошибка при загрузке CatBoostRecommender: {e}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a572906",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGBMRecommender(Recommender):\n",
    "    def __init__(self, model_path: str = \"../models/lightgbm/lightgbm_recommender.pkl\", force_retrain: bool = False):\n",
    "        super().__init__(model_path)\n",
    "        self.vectorizer = None\n",
    "        self.item_vectors = None\n",
    "        self.item_ids = None\n",
    "        self.force_retrain = force_retrain\n",
    "\n",
    "    def fit(self, train: pl.DataFrame, vectorizer: TextVectorizer = None):\n",
    "        if not isinstance(vectorizer, TextVectorizer):\n",
    "            logger.error(\"Аргумент 'vectorizer' должен быть экземпляром TextVectorizer\")\n",
    "            raise TypeError(\"Аргумент 'vectorizer' должен быть экземпляром TextVectorizer\")\n",
    "        if self.model_path and os.path.exists(self.model_path) and not self.force_retrain:\n",
    "            self.load_model()\n",
    "            return\n",
    "\n",
    "        logger.info(\"Начало обучения LightGBMRecommender...\")\n",
    "        logger.info(f\"Использование памяти: {psutil.virtual_memory().percent}%\")\n",
    "\n",
    "        # Используем общий векторизатор\n",
    "        self.vectorizer = vectorizer.vectorizer\n",
    "        self.item_vectors = vectorizer.item_vectors\n",
    "        self.item_ids = vectorizer.item_ids\n",
    "\n",
    "        # Путь для сохранения промежуточных данных\n",
    "        data_path = \"../models/lightgbm/lightgbm_data.pkl\"\n",
    "        if os.path.exists(data_path) and not self.force_retrain:\n",
    "            logger.info(\"Загрузка сохраненных данных для LightGBM...\")\n",
    "            data = joblib.load(data_path)\n",
    "            X, y = data['X'], data['y']\n",
    "        else:\n",
    "            # Подготовка данных для LightGBM с пакетной записью на диск\n",
    "            X, y = [], []\n",
    "            user_items = train.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_pandas()\n",
    "            item_id_to_idx = {item_id: idx for idx, item_id in enumerate(self.item_ids)}\n",
    "            batch_size = 1000  # Обрабатываем по 1000 пользователей за раз\n",
    "            batch_count = 0\n",
    "\n",
    "            for i in tqdm(range(0, len(user_items), batch_size), desc=\"Подготовка данных для LightGBM\"):\n",
    "                batch = user_items[i:i+batch_size]\n",
    "                batch_X, batch_y = [], []\n",
    "                for _, row in batch.iterrows():\n",
    "                    user_id = row[\"user_id\"]\n",
    "                    pos_items = set(row[\"item_id\"])\n",
    "                    neg_items = list(set(self.item_ids) - pos_items)[:min(len(pos_items), 5)]  # Уменьшено до 5\n",
    "\n",
    "                    # Используем разреженные матрицы для экономии памяти\n",
    "                    valid_pos_items = [item for item in pos_items if item in item_id_to_idx]\n",
    "                    user_vector = np.mean(\n",
    "                        [self.item_vectors[item_id_to_idx[item]].toarray()[0] for item in valid_pos_items],\n",
    "                        axis=0\n",
    "                    ) if valid_pos_items else np.zeros(self.item_vectors.shape[1])\n",
    "\n",
    "                    for item_id in pos_items:\n",
    "                        if item_id in item_id_to_idx:\n",
    "                            item_vector = self.item_vectors[item_id_to_idx[item_id]].toarray()[0]\n",
    "                            batch_X.append(np.concatenate([user_vector, item_vector]))\n",
    "                            batch_y.append(1)\n",
    "\n",
    "                    for item_id in neg_items:\n",
    "                        if item_id in item_id_to_idx:\n",
    "                            item_vector = self.item_vectors[item_id_to_idx[item_id]].toarray()[0]\n",
    "                            batch_X.append(np.concatenate([user_vector, item_vector]))\n",
    "                            batch_y.append(0)\n",
    "\n",
    "                # Сохранение батча на диск\n",
    "                batch_path = f\"../models/lightgbm/lightgbm_data_batch_{batch_count}.pkl\"\n",
    "                try:\n",
    "                    os.makedirs(os.path.dirname(batch_path), exist_ok=True)\n",
    "                    joblib.dump({'X': batch_X, 'y': batch_y}, batch_path)\n",
    "                    logger.info(f\"Батч {batch_count} сохранен в {batch_path}\")\n",
    "                    batch_count += 1\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Ошибка при сохранении батча {batch_count}: {e}\")\n",
    "                    raise\n",
    "                # Очистка памяти\n",
    "                batch_X, batch_y = [], []\n",
    "                gc.collect()\n",
    "\n",
    "            # Объединение батчей\n",
    "            X, y = [], []\n",
    "            for i in range(batch_count):\n",
    "                batch_data = joblib.load(f\"../models/lightgbm/lightgbm_data_batch_{i}.pkl\")\n",
    "                X.extend(batch_data['X'])\n",
    "                y.extend(batch_data['y'])\n",
    "                os.remove(f\"../models/lightgbm/lightgbm_data_batch_{i}.pkl\")  # Удаление временного файла\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
    "                joblib.dump({'X': X, 'y': y}, data_path)\n",
    "                logger.info(f\"Данные LightGBM сохранены в {data_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Ошибка при сохранении данных LightGBM: {e}\")\n",
    "                raise\n",
    "\n",
    "        logger.info(f\"Создано {len(X)} пар пользователь-книга\")\n",
    "        logger.info(f\"Использование памяти после подготовки данных: {psutil.virtual_memory().percent}%\")\n",
    "\n",
    "        # Обучение LightGBM\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'binary_logloss',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'num_threads': -1\n",
    "        }\n",
    "        self.model = lgb.train(params, train_data, valid_sets=[val_data], num_boost_round=30)\n",
    "        logger.info(\"Обучение LightGBM завершено\")\n",
    "\n",
    "        self.save_model()\n",
    "\n",
    "    def predict(self, user_ids: List[int], k: int = 10, batch_size: int = 1000) -> Dict[int, List[int]]:\n",
    "        predictions = {}\n",
    "        item_id_to_idx = {item_id: idx for idx, item_id in enumerate(self.item_ids)}\n",
    "\n",
    "        for i in tqdm(range(0, len(user_ids), batch_size), desc=\"Predicting with LightGBM\"):\n",
    "            batch_users = user_ids[i:i+batch_size]\n",
    "            for user_id in batch_users:\n",
    "                user_items = train.filter(pl.col(\"user_id\") == user_id)[\"item_id\"].to_list()\n",
    "                user_vector = np.mean(\n",
    "                    [self.item_vectors[item_id_to_idx[item]].toarray()[0] for item in user_items if item in item_id_to_idx],\n",
    "                    axis=0\n",
    "                ) if user_items else np.zeros(self.item_vectors.shape[1])\n",
    "\n",
    "                X_pred = []\n",
    "                pred_items = []\n",
    "                for item_id in self.item_ids:\n",
    "                    item_vector = self.item_vectors[item_id_to_idx[item_id]].toarray()[0]\n",
    "                    X_pred.append(np.concatenate([user_vector, item_vector]))\n",
    "                    pred_items.append(item_id)\n",
    "\n",
    "                scores = self.model.predict(X_pred)\n",
    "                top_k_indices = np.argsort(scores)[::-1][:k]\n",
    "                predictions[user_id] = [pred_items[idx] for idx in top_k_indices]\n",
    "            gc.collect()  # Очистка памяти после батча\n",
    "        return predictions\n",
    "\n",
    "    def save_model(self):\n",
    "        if self.model_path:\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(self.model_path), exist_ok=True)\n",
    "                joblib.dump({\n",
    "                    'model': self.model,\n",
    "                    'vectorizer': self.vectorizer,\n",
    "                    'item_vectors': self.item_vectors,\n",
    "                    'item_ids': self.item_ids\n",
    "                }, self.model_path)\n",
    "                logger.info(f\"Модель сохранена в {self.model_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Ошибка при сохранении LightGBMRecommender: {e}\")\n",
    "                raise\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model_path and os.path.exists(self.model_path):\n",
    "            try:\n",
    "                data = joblib.load(self.model_path)\n",
    "                self.model = data['model']\n",
    "                self.vectorizer = data['vectorizer']\n",
    "                self.item_vectors = data['item_vectors']\n",
    "                self.item_ids = data['item_ids']\n",
    "                logger.info(f\"Модель LightGBMRecommender загружена из {self.model_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Ошибка при загрузке LightGBMRecommender: {e}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7140281",
   "metadata": {},
   "source": [
    "### Обучение моделей "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301c098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 21:33:48,466 - INFO - Начало TF-IDF векторизации...\n",
      "2025-09-06 21:33:48,467 - INFO - Использование памяти: 58.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train users: 349719, items: 31300\n",
      "Test users: 185828, items: 27367\n",
      "Cold items: 1775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 21:33:51,098 - INFO - TF-IDF векторизация завершена. Размер матрицы: (34322, 1000)\n",
      "2025-09-06 21:33:51,099 - INFO - Использование памяти после векторизации: 58.8%\n",
      "2025-09-06 21:33:51,146 - INFO - Векторизатор сохранен в ../models/vectorizer/tfidf_vectorizer.pkl\n",
      "2025-09-06 21:33:58,234 - INFO - Модель TFIDFRecommender загружена из ../models/tfidf/tfidf_recommender.pkl\n",
      "2025-09-06 21:33:58,838 - INFO - Обучение моделей...\n",
      "2025-09-06 21:33:58,838 - INFO - Начало обучения TFIDFRecommender...\n",
      "2025-09-06 21:33:58,838 - INFO - Использование памяти: 63.0%\n",
      "2025-09-06 21:33:58,839 - ERROR - Ошибка при обучении моделей: 'DataFrame' object has no attribute 'vectorizer'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'vectorizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mОбучение моделей...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mtfidf_recommender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbooks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     catboost_recommender\u001b[38;5;241m.\u001b[39mfit(train, books)\n\u001b[1;32m     25\u001b[0m     lightgbm_recommender\u001b[38;5;241m.\u001b[39mfit(train, books)\n",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m, in \u001b[0;36mTFIDFRecommender.fit\u001b[0;34m(self, train, vectorizer)\u001b[0m\n\u001b[1;32m     16\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mИспользование памяти: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpsutil\u001b[38;5;241m.\u001b[39mvirtual_memory()\u001b[38;5;241m.\u001b[39mpercent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Используем общий векторизатор\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorizer\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_vectors \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mitem_vectors\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_ids \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mitem_ids\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'vectorizer'"
     ]
    }
   ],
   "source": [
    "print(f\"Train users: {train['user_id'].n_unique()}, items: {len(train_items)}\")\n",
    "print(f\"Test users: {test['user_id'].n_unique()}, items: {len(test_items)}\")\n",
    "print(f\"Cold items: {len(cold_items)}\")\n",
    "user_ids = test[\"user_id\"].unique().to_list()\n",
    "\n",
    "vectorizer = TextVectorizer(max_features=1000)\n",
    "vectorizer_path = \"../models/vectorizer/tfidf_vectorizer.pkl\"\n",
    "if os.path.exists(vectorizer_path):\n",
    "    vectorizer.load(vectorizer_path)\n",
    "else:\n",
    "    vectorizer.fit(books)\n",
    "    vectorizer.save(vectorizer_path)\n",
    "\n",
    "# Инициализируем и обучаем разные модели\n",
    "force_retrain = True  # Переключатель для обучения/загрузки моделей\n",
    "tfidf_recommender = TFIDFRecommender(model_path=\"../models/tfidf/tfidf_recommender.pkl\", force_retrain=force_retrain)\n",
    "catboost_recommender = CatBoostRecommender(model_path=\"../models/catboost/catboost_recommender.pkl\", force_retrain=force_retrain)\n",
    "lightgbm_recommender = LightGBMRecommender(model_path=\"../models/lightgbm/lightgbm_recommender.pkl\", force_retrain=force_retrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709672af",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"TF-IDF\": pred_tfidf,\n",
    "    \"CatBoost\": pred_catboost,\n",
    "    \"LightGBM\": pred_gbm,\n",
    "}\n",
    "\n",
    "\n",
    "print(\"\\nРекомендации моделей:\")\n",
    "train_df = show_predictions(models, train, n=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f4b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nСтатистика по метрикам для каждой модели:\")\n",
    "validator = JointValidator(train, test, cold_items)\n",
    "metrics_df = val_predictions(models, test, validator, k=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a9f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nСтатистика по метрикам для каждой модели (SplitValidator):\")\n",
    "split_validator = SplitValidator(train, test, cold_items)\n",
    "metrics_df_split = val_predictions(models, test, split_validator, k=10, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
