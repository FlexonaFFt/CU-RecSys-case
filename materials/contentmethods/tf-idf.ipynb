{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25497418",
   "metadata": {},
   "source": [
    "# Контентные методы (tf-idf) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8688bfe8",
   "metadata": {},
   "source": [
    "### Базовые операции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2afc3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import logging\n",
    "import joblib\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict, Counter \n",
    "\n",
    "from scipy.sparse import vstack, hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "os.makedirs('../models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aecb716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сразу загружу данные\n",
    "train = pl.read_parquet(\"../../data/train.pq\")\n",
    "test = pl.read_parquet(\"../../data/test.pq\")\n",
    "books = pl.read_parquet(\"../../data/books.pq\")\n",
    "\n",
    "train_items = set(train[\"item_id\"].unique())\n",
    "test_items = set(test[\"item_id\"].unique())\n",
    "cold_items = test_items - train_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc86f568",
   "metadata": {},
   "source": [
    "#### Метрики оценивания моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b46c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validator(ABC):\n",
    "    def __init__(self, train: pd.DataFrame, test: pd.DataFrame, cold_items: set = None):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.cold_items = cold_items or set()\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self, predictions: Dict[int, List[int]]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        predictions: dict user_id -> list of recommended item_ids\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def recall_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        return len(set(y_true) & set(y_pred[:k])) / len(set(y_true)) if y_true else 0.0\n",
    "\n",
    "    def precision_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        return len(set(y_true) & set(y_pred[:k])) / k if y_true else 0.0\n",
    "\n",
    "    def hitrate_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        return 1.0 if len(set(y_true) & set(y_pred[:k])) > 0 else 0.0\n",
    "\n",
    "    def ndcg_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        dcg = 0.0\n",
    "        for i, item in enumerate(y_pred[:k]):\n",
    "            if item in y_true:\n",
    "                dcg += 1 / np.log2(i + 2)\n",
    "        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(y_true), k)))\n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    def mrr_at_k(self, y_true: List[int], y_pred: List[int], k: int = 10) -> float:\n",
    "        for i, item in enumerate(y_pred[:k]):\n",
    "            if item in y_true:\n",
    "                return 1 / (i + 1)\n",
    "        return 0.0\n",
    "\n",
    "    def coverage(self, predictions: Dict[int, List[int]]) -> float:\n",
    "        all_pred_items = set(item for recs in predictions.values() for item in recs)\n",
    "        all_train_items = set(self.train[\"item_id\"].unique())\n",
    "        return len(all_pred_items) / len(all_train_items)\n",
    "\n",
    "    @staticmethod\n",
    "    def print_metrics(metrics: Dict[str, float]):\n",
    "        print(\"\\n=== Evaluation Results ===\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key:<15}: {value:.4f}\")\n",
    "        print(\"==========================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e5b41",
   "metadata": {},
   "source": [
    "Стоит разделить валидацию на две версии, для сравнения. Моя гипотеза заключается в том, что совместная валидация warm и cold может быть не совсем честной. Например, если в тесте 90% warm и 10% cold, то Recall@10 в среднем будет определяться warm-айтемами. Модель может полностью «забыть» про cold items, но в отчёте всё равно будут хорошие цифры. Это вводит в заблуждение: кажется, что модель универсальная, хотя на самом деле cold-start не решён."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada31206",
   "metadata": {},
   "source": [
    "#### Блок совместной валидации (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8fd76c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointValidator(Validator):\n",
    "    def __init__(self, train: pl.DataFrame, test: pl.DataFrame, cold_items: set = None):\n",
    "        super().__init__(train, test, cold_items)\n",
    "        self.user2items = (\n",
    "            test.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_dict(as_series=False)\n",
    "        )\n",
    "        self.user2items = dict(zip(self.user2items[\"user_id\"], self.user2items[\"item_id\"]))\n",
    "\n",
    "    def evaluate(self, predictions: Dict[int, List[int]]) -> Dict[str, float]:\n",
    "        recalls, precisions, hits, ndcgs, mrrs = [], [], [], [], []\n",
    "        for user_id, y_pred in predictions.items():\n",
    "            y_true = self.user2items.get(user_id, [])\n",
    "            recalls.append(self.recall_at_k(y_true, y_pred))\n",
    "            precisions.append(self.precision_at_k(y_true, y_pred))\n",
    "            hits.append(self.hitrate_at_k(y_true, y_pred))\n",
    "            ndcgs.append(self.ndcg_at_k(y_true, y_pred))\n",
    "            mrrs.append(self.mrr_at_k(y_true, y_pred))\n",
    "        results = {\n",
    "            \"Recall@10\": np.mean(recalls),\n",
    "            \"Precision@10\": np.mean(precisions),\n",
    "            \"HitRate@10\": np.mean(hits),\n",
    "            \"NDCG@10\": np.mean(ndcgs),\n",
    "            \"MRR@10\": np.mean(mrrs),\n",
    "            \"Coverage\": self.coverage(predictions),\n",
    "        }\n",
    "        self.print_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c9d8c",
   "metadata": {},
   "source": [
    "#### Разделенная валидация (cold vs warm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4fcf8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitValidator(Validator):\n",
    "    def __init__(self, train: pl.DataFrame, test: pl.DataFrame, cold_items: set = None):\n",
    "        super().__init__(train, test, cold_items)\n",
    "        self.user2items = (\n",
    "            test.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_dict(as_series=False)\n",
    "        )\n",
    "        self.user2items = dict(zip(self.user2items[\"user_id\"], self.user2items[\"item_id\"]))\n",
    "\n",
    "    def evaluate(self, predictions: Dict[int, List[int]]) -> Dict[str, float]:\n",
    "        results = {}\n",
    "        for subset in [\"cold\", \"warm\"]:\n",
    "            recalls, precisions, hits, ndcgs, mrrs = [], [], [], [], []\n",
    "            for user_id, y_pred in predictions.items():\n",
    "                y_true = self.user2items.get(user_id, [])\n",
    "                if not y_true:\n",
    "                    continue\n",
    "                if subset == \"cold\":\n",
    "                    y_true = [i for i in y_true if i in self.cold_items]\n",
    "                elif subset == \"warm\":\n",
    "                    y_true = [i for i in y_true if i not in self.cold_items]\n",
    "                if not y_true:\n",
    "                    continue\n",
    "                recalls.append(self.recall_at_k(y_true, y_pred))\n",
    "                precisions.append(self.precision_at_k(y_true, y_pred))\n",
    "                hits.append(self.hitrate_at_k(y_true, y_pred))\n",
    "                ndcgs.append(self.ndcg_at_k(y_true, y_pred))\n",
    "                mrrs.append(self.mrr_at_k(y_true, y_pred))\n",
    "            results[f\"Recall@10_{subset}\"] = np.mean(recalls) if recalls else 0.0\n",
    "            results[f\"Precision@10_{subset}\"] = np.mean(precisions) if precisions else 0.0\n",
    "            results[f\"HitRate@10_{subset}\"] = np.mean(hits) if hits else 0.0\n",
    "            results[f\"NDCG@10_{subset}\"] = np.mean(ndcgs) if ndcgs else 0.0\n",
    "            results[f\"MRR@10_{subset}\"] = np.mean(mrrs) if mrrs else 0.0\n",
    "        results[\"Coverage\"] = self.coverage(predictions)\n",
    "        self.print_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5488f3",
   "metadata": {},
   "source": [
    "#### Вспомогательные функции для удобного представления результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37d934e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shorten_list(lst, max_len=10):\n",
    "    \"\"\"Обрезает длинные списки для красивого вывода\"\"\"\n",
    "    if lst is None:\n",
    "        return []\n",
    "    return lst[:max_len] if len(lst) > max_len else lst\n",
    "\n",
    "def show_predictions(models: dict, data: pl.DataFrame, n=5, verbose=True, is_val=False):\n",
    "    df = data.sample(n).select([\"user_id\", \"item_id\"])\n",
    "    if is_val:\n",
    "        df = df.rename({\"item_id\": \"true_items\"})\n",
    "\n",
    "    # добавляем предсказания\n",
    "    for name, preds in models.items():\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"user_id\").map_elements(\n",
    "                lambda u: _shorten_list(preds.get(u, [])), \n",
    "                return_dtype=pl.List(pl.Int64)\n",
    "            ).alias(name)\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        print(df.shape)\n",
    "        print(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def val_predictions(models: dict, val: pl.DataFrame, validator: Validator, k: int = 10, verbose: bool = True):\n",
    "    results = []\n",
    "    user2items = (\n",
    "        val.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_dict(as_series=False)\n",
    "    )\n",
    "    user2items = dict(zip(user2items[\"user_id\"], user2items[\"item_id\"]))\n",
    "\n",
    "    for model_name, preds in models.items():\n",
    "        recalls, precisions, hits, ndcgs, mrrs = [], [], [], [], []\n",
    "        for u, y_true in user2items.items():\n",
    "            y_pred = preds.get(u, [])\n",
    "            recalls.append(validator.recall_at_k(y_true, y_pred, k))\n",
    "            precisions.append(validator.precision_at_k(y_true, y_pred, k))\n",
    "            hits.append(validator.hitrate_at_k(y_true, y_pred, k))\n",
    "            ndcgs.append(validator.ndcg_at_k(y_true, y_pred, k))\n",
    "            mrrs.append(validator.mrr_at_k(y_true, y_pred, k))\n",
    "        metrics = {\n",
    "            \"model\": model_name,\n",
    "            \"Recall@10\": np.mean(recalls),\n",
    "            \"Precision@10\": np.mean(precisions),\n",
    "            \"HitRate@10\": np.mean(hits),\n",
    "            \"NDCG@10\": np.mean(ndcgs),\n",
    "            \"MRR@10\": np.mean(mrrs),\n",
    "            \"Coverage\": validator.coverage(preds),\n",
    "        }\n",
    "        results.append(metrics)\n",
    "\n",
    "    df = pl.DataFrame(results)\n",
    "    if verbose:\n",
    "        print(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624119e0",
   "metadata": {},
   "source": [
    "#### Функция векторизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5fb4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorizer:\n",
    "    def __init__(self, max_features: int = 200, stop_words: str = 'english'):  # Уменьшено до 200\n",
    "        self.vectorizer = TfidfVectorizer(max_features=max_features, stop_words=stop_words)\n",
    "        self.item_vectors = None\n",
    "        self.item_ids = None\n",
    "\n",
    "    def fit(self, metadata: pl.DataFrame):\n",
    "        logger.info(\"Начало TF-IDF векторизации...\")\n",
    "        logger.info(f\"Использование памяти: {psutil.virtual_memory().percent}%\")\n",
    "        \n",
    "        # Подготовка текстовых данных\n",
    "        metadata = metadata.with_columns(\n",
    "            pl.col(\"description\").fill_null(\"\") + \" \" + \n",
    "            pl.col(\"tags\").list.join(\" \").fill_null(\"\")\n",
    "        )\n",
    "        corpus = metadata[\"description\"].to_list()\n",
    "        self.item_ids = metadata[\"item_id\"].to_list()\n",
    "\n",
    "        # Векторизация\n",
    "        self.item_vectors = self.vectorizer.fit_transform(corpus)\n",
    "        logger.info(f\"TF-IDF векторизация завершена. Размер матрицы: {self.item_vectors.shape}\")\n",
    "        logger.info(f\"Использование памяти после векторизации: {psutil.virtual_memory().percent}%\")\n",
    "\n",
    "    def save(self, path: str):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "            joblib.dump({\n",
    "                'vectorizer': self.vectorizer,\n",
    "                'item_vectors': self.item_vectors,\n",
    "                'item_ids': self.item_ids\n",
    "            }, path)\n",
    "            logger.info(f\"Векторизатор сохранен в {path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка при сохранении векторизатора: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load(self, path: str):\n",
    "        try:\n",
    "            data = joblib.load(path)\n",
    "            self.vectorizer = data['vectorizer']\n",
    "            self.item_vectors = data['item_vectors']\n",
    "            self.item_ids = data['item_ids']\n",
    "            logger.info(f\"Векторизатор загружен из {path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка при загрузке векторизатора: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289159ad",
   "metadata": {},
   "source": [
    "#### Базовый класс для загрузки моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04fc2c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender(ABC):\n",
    "    def __init__(self, model_path: str = None):\n",
    "        self.model = None\n",
    "        self.model_path = model_path\n",
    "        if model_path and os.path.exists(model_path):\n",
    "            self.load_model()\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, train: pl.DataFrame, vectorizer: TextVectorizer = None):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, user_ids: List[int], k: int = 10) -> Dict[int, List[int]]:\n",
    "        pass\n",
    "\n",
    "    def save_model(self):\n",
    "        if self.model_path:\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(self.model_path), exist_ok=True)\n",
    "                joblib.dump(self.model, self.model_path)\n",
    "                logger.info(f\"Модель сохранена в {self.model_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Ошибка при сохранении модели в {self.model_path}: {e}\")\n",
    "                raise\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model_path and os.path.exists(self.model_path):\n",
    "            try:\n",
    "                self.model = joblib.load(self.model_path)\n",
    "                logger.info(f\"Модель загружена из {self.model_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Ошибка при загрузке модели из {self.model_path}: {e}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bce9b2",
   "metadata": {},
   "source": [
    "### Использование контентных методов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c501b",
   "metadata": {},
   "source": [
    "#### TF-IDF на дополнительных данных\n",
    "\n",
    "Будем представлять данные в векторном формате (description, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b625394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFRecommender(Recommender):  # Assumes Recommender base class is defined\n",
    "    def __init__(self, model_path: str = \"../models/tfidf/tfidf_recommender.pkl\", force_retrain: bool = False):\n",
    "        super().__init__(model_path)\n",
    "        self.vectorizer = None\n",
    "        self.item_vectors = None\n",
    "        self.item_ids = None\n",
    "        self.user_profiles = None\n",
    "        self.force_retrain = force_retrain\n",
    "\n",
    "    def fit(self, train: pl.DataFrame, vectorizer: TextVectorizer = None):\n",
    "        if not isinstance(vectorizer, TextVectorizer):\n",
    "            logger.error(\"Аргумент 'vectorizer' должен быть экземпляром TextVectorizer\")\n",
    "            raise TypeError(\"Аргумент 'vectorizer' должен быть экземпляром TextVectorizer\")\n",
    "        if self.model_path and os.path.exists(self.model_path) and not self.force_retrain:\n",
    "            self.load_model()\n",
    "            return\n",
    "\n",
    "        logger.info(\"Начало обучения TFIDFRecommender...\")\n",
    "        logger.info(f\"Использование памяти: {psutil.virtual_memory().percent}%\")\n",
    "\n",
    "        self.vectorizer = vectorizer.vectorizer\n",
    "        self.item_vectors = vectorizer.item_vectors\n",
    "        self.item_ids = vectorizer.item_ids\n",
    "\n",
    "        # Create user profiles\n",
    "        user_items = train.group_by(\"user_id\").agg(pl.col(\"item_id\")).to_pandas()\n",
    "        item_id_to_idx = {item_id: idx for idx, item_id in enumerate(self.item_ids)}\n",
    "        self.user_profiles = {}\n",
    "        \n",
    "        for _, row in user_items.iterrows():\n",
    "            user_id = row[\"user_id\"]\n",
    "            valid_items = [item for item in row[\"item_id\"] if item in item_id_to_idx]\n",
    "            if valid_items:\n",
    "                user_vector = vstack([self.item_vectors[item_id_to_idx[item]] for item in valid_items]).mean(axis=0)\n",
    "                self.user_profiles[user_id] = user_vector\n",
    "            else:\n",
    "                self.user_profiles[user_id] = np.zeros((1, self.item_vectors.shape[1]))\n",
    "\n",
    "        logger.info(\"Профили пользователей созданы\")\n",
    "        logger.info(f\"Использование памяти после создания профилей: {psutil.virtual_memory().percent}%\")\n",
    "        self.save_model()\n",
    "\n",
    "    def predict(self, user_ids: List[int], k: int = 10, item_subset: List[int] = None) -> Dict[int, List[int]]:\n",
    "        predictions = {}\n",
    "        item_id_to_idx = {item_id: idx for idx, item_id in enumerate(self.item_ids)}\n",
    "        pred_items_subset = item_subset if item_subset is not None else self.item_ids\n",
    "\n",
    "        for user_id in tqdm(user_ids, desc=\"Predicting with TFIDF\"):\n",
    "            if user_id in self.user_profiles:\n",
    "                user_vector = self.user_profiles[user_id]\n",
    "            else:\n",
    "                user_items = train.filter(pl.col(\"user_id\") == user_id)[\"item_id\"].to_list()\n",
    "                valid_items = [item for item in user_items if item in item_id_to_idx]\n",
    "                user_vector = vstack([self.item_vectors[item_id_to_idx[item]] for item in valid_items]).mean(axis=0) if valid_items else np.zeros((1, self.item_vectors.shape[1]))\n",
    "\n",
    "            valid_item_indices = [item_id_to_idx[item_id] for item_id in pred_items_subset if item_id in item_id_to_idx]\n",
    "            item_vectors_subset = self.item_vectors[valid_item_indices]\n",
    "            \n",
    "            user_vector_array = np.asarray(user_vector)\n",
    "            scores = cosine_similarity(user_vector_array, item_vectors_subset).flatten()\n",
    "            top_k_indices = np.argsort(scores)[::-1][:k]\n",
    "            predictions[user_id] = [pred_items_subset[idx] for idx in top_k_indices]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def save_model(self):\n",
    "        if self.model_path:\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(self.model_path), exist_ok=True)\n",
    "                joblib.dump({\n",
    "                    'model': self,\n",
    "                    'vectorizer': self.vectorizer,\n",
    "                    'item_vectors': self.item_vectors,\n",
    "                    'item_ids': self.item_ids,\n",
    "                    'user_profiles': self.user_profiles\n",
    "                }, self.model_path)\n",
    "                logger.info(f\"Модель сохранена в {self.model_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Ошибка при сохранении TFIDFRecommender: {e}\")\n",
    "                raise\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model_path and os.path.exists(self.model_path):\n",
    "            try:\n",
    "                data = joblib.load(self.model_path)\n",
    "                self.vectorizer = data['vectorizer']\n",
    "                self.item_vectors = data['item_vectors']\n",
    "                self.item_ids = data['item_ids']\n",
    "                self.user_profiles = data['user_profiles']\n",
    "                logger.info(f\"Модель TFIDFRecommender загружена из {self.model_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Ошибка при загрузке TFIDFRecommender: {e}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7140281",
   "metadata": {},
   "source": [
    "### Обучение моделей "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3301c098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 11:13:49,403 - INFO - Начало TF-IDF векторизации...\n",
      "2025-09-07 11:13:49,403 - INFO - Использование памяти: 63.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train users: 349719, items: 31300\n",
      "Test users: 185828, items: 27367\n",
      "Cold items: 1775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 11:13:51,994 - INFO - TF-IDF векторизация завершена. Размер матрицы: (34322, 1000)\n",
      "2025-09-07 11:13:51,995 - INFO - Использование памяти после векторизации: 63.4%\n",
      "2025-09-07 11:13:52,038 - INFO - Векторизатор сохранен в ../models/vectorizer/tfidf_vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train users: {train['user_id'].n_unique()}, items: {len(train_items)}\")\n",
    "print(f\"Test users: {test['user_id'].n_unique()}, items: {len(test_items)}\")\n",
    "print(f\"Cold items: {len(cold_items)}\")\n",
    "user_ids = test[\"user_id\"].unique().to_list()\n",
    "\n",
    "vectorizer = TextVectorizer(max_features=1000)\n",
    "vectorizer_path = \"../models/vectorizer/tfidf_vectorizer.pkl\"\n",
    "if os.path.exists(vectorizer_path):\n",
    "    vectorizer.load(vectorizer_path)\n",
    "else:\n",
    "    vectorizer.fit(books)\n",
    "    vectorizer.save(vectorizer_path)\n",
    "\n",
    "# Инициализируем и обучаем разные модели\n",
    "force_retrain = True  # Переключатель для обучения/загрузки моделей\n",
    "tfidf_recommender = TFIDFRecommender(model_path=\"../models/tfidf/tfidf_recommender.pkl\", force_retrain=force_retrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709672af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 11:13:52,301 - INFO - Используется подвыборка для обучения: 34971 пользователей\n",
      "2025-09-07 11:13:52,334 - INFO - Используется подвыборка для предсказания: 18582 пользователей\n",
      "2025-09-07 11:13:52,474 - INFO - === Обучение и предсказание TFIDFRecommender ===\n",
      "2025-09-07 11:13:52,475 - INFO - Начало обучения TFIDFRecommender...\n",
      "2025-09-07 11:13:52,475 - INFO - Использование памяти: 63.6%\n",
      "2025-09-07 11:14:20,055 - INFO - Профили пользователей созданы\n",
      "2025-09-07 11:14:20,055 - INFO - Использование памяти после создания профилей: 63.2%\n",
      "2025-09-07 11:14:21,158 - INFO - Модель сохранена в ../models/tfidf/tfidf_recommender.pkl\n",
      "Predicting with TFIDF: 100%|██████████| 18582/18582 [01:10<00:00, 263.46it/s]\n",
      "2025-09-07 11:15:31,693 - INFO - TFIDFRecommender обучен и предсказания получены\n",
      "2025-09-07 11:15:31,904 - INFO - Предсказания TFIDFRecommender сохранены в ../models/tfidf/pred_tfidf.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Рекомендации модели:\n",
      "(5, 3)\n",
      "shape: (5, 3)\n",
      "┌─────────────────────────────────┬─────────┬─────────────────────────┐\n",
      "│ user_id                         ┆ item_id ┆ TF-IDF                  │\n",
      "│ ---                             ┆ ---     ┆ ---                     │\n",
      "│ str                             ┆ i64     ┆ list[i64]               │\n",
      "╞═════════════════════════════════╪═════════╪═════════════════════════╡\n",
      "│ b2b770716941c4aab7227ac62230cc… ┆ 8508    ┆ []                      │\n",
      "│ 6e24d535ceddbbc23b8fcaaf31661a… ┆ 5943    ┆ []                      │\n",
      "│ 8a544734b8b356f2d3eac55c953c1f… ┆ 33370   ┆ []                      │\n",
      "│ c1b693b9270c8c1dc18bb1f1ae3039… ┆ 1164    ┆ []                      │\n",
      "│ fb2b4f989b5048258a49f20fd2f415… ┆ 20725   ┆ [18881, 34056, … 17256] │\n",
      "└─────────────────────────────────┴─────────┴─────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Подвыборка пользователей для обучения (10% от всех пользователей в train)\n",
    "sample_users = train[\"user_id\"].unique().sample(fraction=0.1, seed=42).to_list()\n",
    "train_sample = train.filter(pl.col(\"user_id\").is_in(sample_users))\n",
    "logger.info(f\"Используется подвыборка для обучения: {len(sample_users)} пользователей\")\n",
    "\n",
    "# Подвыборка пользователей для предсказания (10% от всех пользователей в test)\n",
    "user_ids = test[\"user_id\"].unique().sample(fraction=0.1, seed=42).to_list()\n",
    "logger.info(f\"Используется подвыборка для предсказания: {len(user_ids)} пользователей\")\n",
    "\n",
    "# Ограничение книг для предсказания (топ-1000 популярных)\n",
    "popular_items = train.group_by(\"item_id\").agg(pl.len()).sort(\"len\", descending=True).head(1000)[\"item_id\"].to_list()\n",
    "logger.info(\"=== Обучение и предсказание TFIDFRecommender ===\")\n",
    "pred_tfidf_path = \"../models/tfidf/pred_tfidf.pkl\"\n",
    "if os.path.exists(pred_tfidf_path):\n",
    "    logger.info(\"Загрузка сохраненных предсказаний TFIDFRecommender...\")\n",
    "    pred_tfidf = joblib.load(pred_tfidf_path)\n",
    "else:\n",
    "    try:\n",
    "        tfidf_recommender = TFIDFRecommender(model_path=\"../models/tfidf/tfidf_recommender.pkl\", force_retrain=True)\n",
    "        tfidf_recommender.fit(train_sample, vectorizer)\n",
    "        pred_tfidf = tfidf_recommender.predict(user_ids, item_subset=popular_items)\n",
    "        logger.info(\"TFIDFRecommender обучен и предсказания получены\")\n",
    "        joblib.dump(pred_tfidf, pred_tfidf_path)\n",
    "        logger.info(f\"Предсказания TFIDFRecommender сохранены в {pred_tfidf_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка при обучении/предсказании TFIDFRecommender: {e}\")\n",
    "        raise\n",
    "\n",
    "models = {\n",
    "    \"TF-IDF\": pred_tfidf,\n",
    "}\n",
    "\n",
    "print(\"\\nРекомендации модели:\")\n",
    "train_df = show_predictions(models, train, n=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "970f4b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Статистика по метрикам:\n",
      "shape: (1, 7)\n",
      "┌────────┬───────────┬──────────────┬────────────┬──────────┬──────────┬──────────┐\n",
      "│ model  ┆ Recall@10 ┆ Precision@10 ┆ HitRate@10 ┆ NDCG@10  ┆ MRR@10   ┆ Coverage │\n",
      "│ ---    ┆ ---       ┆ ---          ┆ ---        ┆ ---      ┆ ---      ┆ ---      │\n",
      "│ str    ┆ f64       ┆ f64          ┆ f64        ┆ f64      ┆ f64      ┆ f64      │\n",
      "╞════════╪═══════════╪══════════════╪════════════╪══════════╪══════════╪══════════╡\n",
      "│ TF-IDF ┆ 0.001759  ┆ 0.001657     ┆ 0.012253   ┆ 0.001915 ┆ 0.003431 ┆ 0.031789 │\n",
      "└────────┴───────────┴──────────────┴────────────┴──────────┴──────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nСтатистика по метрикам:\")\n",
    "validator = JointValidator(train, test, cold_items)\n",
    "metrics_df = val_predictions(models, test, validator, k=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0a9f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Статистика по метрикам (SplitValidator):\n",
      "shape: (1, 7)\n",
      "┌────────┬───────────┬──────────────┬────────────┬──────────┬──────────┬──────────┐\n",
      "│ model  ┆ Recall@10 ┆ Precision@10 ┆ HitRate@10 ┆ NDCG@10  ┆ MRR@10   ┆ Coverage │\n",
      "│ ---    ┆ ---       ┆ ---          ┆ ---        ┆ ---      ┆ ---      ┆ ---      │\n",
      "│ str    ┆ f64       ┆ f64          ┆ f64        ┆ f64      ┆ f64      ┆ f64      │\n",
      "╞════════╪═══════════╪══════════════╪════════════╪══════════╪══════════╪══════════╡\n",
      "│ TF-IDF ┆ 0.001759  ┆ 0.001657     ┆ 0.012253   ┆ 0.001915 ┆ 0.003431 ┆ 0.031789 │\n",
      "└────────┴───────────┴──────────────┴────────────┴──────────┴──────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nСтатистика по метрикам (SplitValidator):\")\n",
    "split_validator = SplitValidator(train, test, cold_items)\n",
    "metrics_df_split = val_predictions(models, test, split_validator, k=10, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
