{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2bed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from catboost import CatBoostRanker\n",
    "from lightgbm import LGBMRanker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a465c0eb",
   "metadata": {},
   "source": [
    "## Recommender функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender:\n",
    "    def __init__(self, data_path=\"data/\"):\n",
    "        self.data_path = data_path\n",
    "        self.books = pl.read_parquet(f\"{data_path}books.pq\")\n",
    "        self.train_interactions = pl.read_parquet(f\"{data_path}train.pq\")\n",
    "        self.test_interactions = pl.read_parquet(f\"{data_path}test.pq\")\n",
    "        self.user_map = {uid: i for i, uid in enumerate(self.train_interactions[\"user_id\"].unique())}\n",
    "        self.item_map = {iid: i for i, iid in enumerate(self.books[\"item_id\"].unique())}\n",
    "        self.n_users = len(self.user_map)\n",
    "        self.n_items = len(self.item_map)\n",
    "        self.models = {}\n",
    "\n",
    "    def _clear_memory(self):\n",
    "        gc.collect()\n",
    "        self.books = None\n",
    "        self.train_interactions = None\n",
    "        self.test_interactions = None\n",
    "\n",
    "    def _prepare_features(self):\n",
    "        tag_counts = self.books[\"tags\"].explode().value_counts()\n",
    "        top_tags = tag_counts.filter(pl.col(\"count\") > 100)[\"tags\"].to_list()\n",
    "        self.books = self.books.with_columns([\n",
    "            pl.col(\"tags\").list.set_intersection(top_tags).alias(\"filtered_tags\")\n",
    "        ])\n",
    "        return self.books\n",
    "\n",
    "    def save_model(self, model_name, model):\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        joblib.dump(model, f\"models/{model_name}.joblib\")\n",
    "\n",
    "    def save_predictions(self, predictions, file_name):\n",
    "        os.makedirs(\"predictions\", exist_ok=True)\n",
    "        pl.DataFrame({\"user_id\": predictions[\"user_id\"], \"item_id\": predictions[\"item_id\"], \"score\": predictions[\"score\"]}) \\\n",
    "            .write_parquet(f\"predictions/{file_name}.pq\")\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_predictions(test_data, pred_data, k=10):\n",
    "        # Ground truth: items each user interacted with in test set\n",
    "        test_dict = defaultdict(set)\n",
    "        for row in test_data.iter_rows(named=True):\n",
    "            test_dict[row[\"user_id\"]].add(row[\"item_id\"])\n",
    "\n",
    "        # Sort predictions by score and take top-k\n",
    "        pred_data = pred_data.sort([\"user_id\", \"score\"], descending=[False, True])\n",
    "        pred_dict = defaultdict(list)\n",
    "        for row in pred_data.group_by(\"user_id\").head(k).iter_rows(named=True):\n",
    "            pred_dict[row[\"user_id\"]].append(row[\"item_id\"])\n",
    "\n",
    "        # Metrics\n",
    "        precision = recall = hit_rate = ndcg = mrr = coverage = 0.0\n",
    "        n_users = len(test_dict)\n",
    "        relevant_items = set(test_data[\"item_id\"].unique())\n",
    "\n",
    "        for user_id, true_items in test_dict.items():\n",
    "            pred_items = pred_dict.get(user_id, [])\n",
    "            hits = len(set(pred_items) & true_items)\n",
    "            precision += hits / min(k, len(pred_items)) if pred_items else 0\n",
    "            recall += hits / len(true_items) if true_items else 0\n",
    "            hit_rate += 1 if hits > 0 else 0\n",
    "            if pred_items:\n",
    "                dcg = 0.0\n",
    "                idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(true_items), k)))\n",
    "                for i, item in enumerate(pred_items[:k]):\n",
    "                    if item in true_items:\n",
    "                        dcg += 1.0 / np.log2(i + 2)\n",
    "                ndcg += dcg / idcg if idcg > 0 else 0\n",
    "                for i, item in enumerate(pred_items[:k], 1):\n",
    "                    if item in true_items:\n",
    "                        mrr += 1.0 / i\n",
    "                        break\n",
    "            coverage += len(set(pred_items) & relevant_items) / len(relevant_items) if relevant_items else 0\n",
    "\n",
    "        return {\n",
    "            \"Precision@10\": precision / n_users,\n",
    "            \"Recall@10\": recall / n_users,\n",
    "            \"HitRate@10\": hit_rate / n_users,\n",
    "            \"NDCG@10\": ndcg / n_users,\n",
    "            \"MRR@10\": mrr / n_users,\n",
    "            \"Coverage\": coverage / n_users\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd52eda",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b201d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALSRecommender(Recommender):\n",
    "    def train(self):\n",
    "        books = self._prepare_features()\n",
    "        interactions = self.train_interactions.join(books.select([\"item_id\", \"filtered_tags\"]), on=\"item_id\")\n",
    "        user_ids = interactions[\"user_id\"].apply(self.user_map.get)\n",
    "        item_ids = interactions[\"item_id\"].apply(self.item_map.get)\n",
    "        ratings = interactions[\"rating\"].fill_null(0).to_numpy()\n",
    "        rows = user_ids.to_numpy()\n",
    "        cols = item_ids.to_numpy()\n",
    "        R = csr_matrix((ratings, (rows, cols)), shape=(self.n_users, self.n_items))\n",
    "        from implicit.als import AlternatingLeastSquares\n",
    "        model = AlternatingLeastSquares(factors=50, iterations=15)\n",
    "        with tqdm(total=15, desc=\"ALS Training\") as pbar:\n",
    "            for _ in range(15):\n",
    "                model.fit(R)\n",
    "                pbar.update(1)\n",
    "        self.models[\"ALS\"] = model\n",
    "        self.save_model(\"ALS\", model)\n",
    "        return model\n",
    "\n",
    "    def predict(self):\n",
    "        model = self.models[\"ALS\"]\n",
    "        test_users = self.test_interactions[\"user_id\"].apply(self.user_map.get).to_numpy()\n",
    "        test_items = self.test_interactions[\"item_id\"].apply(self.item_map.get).to_numpy()\n",
    "        scores = model.predict(test_users, test_items)\n",
    "        predictions = pl.DataFrame({\n",
    "            \"user_id\": self.test_interactions[\"user_id\"],\n",
    "            \"item_id\": self.test_interactions[\"item_id\"],\n",
    "            \"score\": scores\n",
    "        })\n",
    "        self.save_predictions(predictions, \"ALS_predictions\")\n",
    "        self._clear_memory()\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aca7018",
   "metadata": {},
   "source": [
    "## EASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ca7110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EASERecommender(Recommender):\n",
    "    def train(self):\n",
    "        books = self._prepare_features()\n",
    "        interactions = self.train_interactions.join(books.select([\"item_id\", \"filtered_tags\"]), on=\"item_id\")\n",
    "        user_ids = interactions[\"user_id\"].apply(self.user_map.get)\n",
    "        item_ids = interactions[\"item_id\"].apply(self.item_map.get)\n",
    "        ratings = interactions[\"rating\"].fill_null(0).to_numpy()\n",
    "        R = csr_matrix((ratings, (user_ids.to_numpy(), item_ids.to_numpy())), shape=(self.n_users, self.n_items))\n",
    "        from implicit.evaluation import train_test_split\n",
    "        R_train, R_test = train_test_split(R, train_percentage=1.0)\n",
    "        # Simplified EASE (placeholder, replace with actual implementation)\n",
    "        self.models[\"EASE\"] = None\n",
    "        with tqdm(total=1, desc=\"EASE Training\") as pbar:\n",
    "            pbar.update(1)  # Placeholder progress\n",
    "        self.save_model(\"EASE\", None)\n",
    "        return None\n",
    "\n",
    "    def predict(self):\n",
    "        test_users = self.test_interactions[\"user_id\"].apply(self.user_map.get).to_numpy()\n",
    "        test_items = self.test_interactions[\"item_id\"].apply(self.item_map.get).to_numpy()\n",
    "        scores = np.random.rand(len(test_users))\n",
    "        predictions = pl.DataFrame({\n",
    "            \"user_id\": self.test_interactions[\"user_id\"],\n",
    "            \"item_id\": self.test_interactions[\"item_id\"],\n",
    "            \"score\": scores\n",
    "        })\n",
    "        self.save_predictions(predictions, \"EASE_predictions\")\n",
    "        self._clear_memory()\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c3618",
   "metadata": {},
   "source": [
    "## LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightFMRecommender(Recommender):\n",
    "    def train(self):\n",
    "        books = self._prepare_features()\n",
    "        dataset = Dataset()\n",
    "        dataset.fit(\n",
    "            users=self.train_interactions[\"user_id\"].unique(),\n",
    "            items=self.books[\"item_id\"].unique(),\n",
    "            item_features=[(tag, [1]) for tag in books[\"filtered_tags\"].explode().unique()]\n",
    "        )\n",
    "        interactions = self.train_interactions.join(books.select([\"item_id\", \"filtered_tags\"]), on=\"item_id\")\n",
    "        (interactions_mat, weights) = dataset.build_interactions(\n",
    "            [(uid, iid, r) for uid, iid, r in zip(interactions[\"user_id\"], interactions[\"item_id\"], interactions[\"rating\"])]\n",
    "        )\n",
    "        item_features = dataset.build_item_features(\n",
    "            [(iid, tags) for iid, tags in zip(books[\"item_id\"], books[\"filtered_tags\"])]\n",
    "        )\n",
    "        model = LightFM(no_components=50, loss='warp')\n",
    "        with tqdm(total=15, desc=\"LightFM Training\") as pbar:\n",
    "            for epoch in range(15):\n",
    "                model.fit_partial(interactions_mat, item_features=item_features, epochs=1)\n",
    "                pbar.update(1)\n",
    "        self.models[\"LightFM\"] = model\n",
    "        self.save_model(\"LightFM\", model)\n",
    "        return model\n",
    "\n",
    "    def predict(self):\n",
    "        model = self.models[\"LightFM\"]\n",
    "        test_users = self.test_interactions[\"user_id\"].to_list()\n",
    "        test_items = self.test_interactions[\"item_id\"].to_list()\n",
    "        scores = model.predict([self.user_map[u] for u in test_users], [self.item_map[i] for i in test_items])\n",
    "        predictions = pl.DataFrame({\n",
    "            \"user_id\": test_users,\n",
    "            \"item_id\": test_items,\n",
    "            \"score\": scores\n",
    "        })\n",
    "        self.save_predictions(predictions, \"LightFM_predictions\")\n",
    "        self._clear_memory()\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a623b593",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f9bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatBoostLTRRecommender(Recommender):\n",
    "    def train(self):\n",
    "        books = self._prepare_features()\n",
    "        interactions = self.train_interactions.join(books.select([\"item_id\", \"filtered_tags\"]), on=\"item_id\")\n",
    "        groups = interactions.group_by(\"user_id\").agg(pl.len()).sort(\"user_id\")[\"len\"].to_list()\n",
    "        X = interactions.select([\"item_id\", \"filtered_tags\", \"is_read\", \"rating\"]).to_pandas()\n",
    "        y = X.pop(\"rating\")\n",
    "        model = CatBoostRanker(iterations=100, learning_rate=0.1, loss_function='YetiRank')\n",
    "        with tqdm(total=100, desc=\"CatBoostLTR Training\") as pbar:\n",
    "            model.fit(X, y, group_id=groups, verbose=False, callback=lambda x: pbar.update(1))\n",
    "        self.models[\"CatBoostLTR\"] = model\n",
    "        self.save_model(\"CatBoostLTR\", model)\n",
    "        return model\n",
    "\n",
    "    def predict(self):\n",
    "        model = self.models[\"CatBoostLTR\"]\n",
    "        test_data = self.test_interactions.join(self.books.select([\"item_id\", \"filtered_tags\"]), on=\"item_id\")\n",
    "        X_test = test_data.select([\"item_id\", \"filtered_tags\", \"is_read\"]).to_pandas()\n",
    "        scores = model.predict(X_test)\n",
    "        predictions = pl.DataFrame({\n",
    "            \"user_id\": test_data[\"user_id\"],\n",
    "            \"item_id\": test_data[\"item_id\"],\n",
    "            \"score\": scores\n",
    "        })\n",
    "        self.save_predictions(predictions, \"CatBoostLTR_predictions\")\n",
    "        self._clear_memory()\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb2341",
   "metadata": {},
   "source": [
    "## Train Intitial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e7ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_models(data_path=\"data/\", pred_path=\"predictions/\"):\n",
    "    test_data = pl.read_parquet(f\"{data_path}test.pq\")\n",
    "    models = [\"ALS\", \"EASE\", \"LightFM\", \"CatBoostLTR\"]\n",
    "    results = []\n",
    "\n",
    "    for model in models:\n",
    "        pred_data = pl.read_parquet(f\"{pred_path}{model}_predictions.pq\")\n",
    "        metrics = Recommender.evaluate_predictions(test_data, pred_data)\n",
    "        results.append({\"model\": model, **metrics})\n",
    "\n",
    "    df = pl.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f3083",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender_types = [ALSRecommender, EASERecommender, LightFMRecommender, CatBoostLTRRecommender]\n",
    "for recommender_class in recommender_types:\n",
    "    recommender = recommender_class()\n",
    "    print(f\"Training {recommender_class.__name__}\")\n",
    "    recommender.train()\n",
    "    print(f\"Predicting with {recommender_class.__name__}\")\n",
    "    recommender.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b2bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluating Models:')\n",
    "metrix_df = evaluate_all_models()\n",
    "print(metrix_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
