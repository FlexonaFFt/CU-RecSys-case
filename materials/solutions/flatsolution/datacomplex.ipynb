{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda0548a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flexonafft/curecsys/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "try:\n",
    "    import implicit\n",
    "    HAS_IMPLICIT = True\n",
    "except Exception:\n",
    "    HAS_IMPLICIT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c3e5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../../../data/\"\n",
    "train = pl.read_parquet(data_folder + \"train.pq\")\n",
    "test_exploded = pl.read_parquet(data_folder + \"test.pq\")\n",
    "test = test_exploded.group_by(\"user_id\", maintain_order=True).agg(pl.col(\"item_id\"))\n",
    "books = pl.read_parquet(data_folder + \"books.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d097e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = train.to_pandas()\n",
    "test_pd = test.to_pandas()\n",
    "books_pd = books.to_pandas()\n",
    "\n",
    "# Приведение типов\n",
    "# Преобразуем user_id и item_id в строку (если вдруг числа — всё равно ок)\n",
    "train_pd[\"user_id\"] = train_pd[\"user_id\"].astype(str)\n",
    "train_pd[\"item_id\"] = train_pd[\"item_id\"].astype(str)\n",
    "test_pd[\"user_id\"] = test_pd[\"user_id\"].astype(str)\n",
    "books_pd[\"item_id\"] = books_pd[\"item_id\"].astype(str)\n",
    "\n",
    "# Собираем уникальные id\n",
    "user_ids = train_pd[\"user_id\"].unique()\n",
    "item_ids = train_pd[\"item_id\"].unique()\n",
    "\n",
    "# Маппинги\n",
    "user_to_idx = {u: i for i, u in enumerate(user_ids)}\n",
    "idx_to_user = {i: u for u, i in user_to_idx.items()}\n",
    "item_to_idx = {i: j for j, i in enumerate(item_ids)}\n",
    "idx_to_item = {j: i for i, j in item_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3220b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularityRecommender:\n",
    "    def __init__(self):\n",
    "        self.counter = Counter()\n",
    "\n",
    "    def partial_fit(self, df: pd.DataFrame):\n",
    "        self.counter.update(df[\"item_id\"].tolist())\n",
    "        return self\n",
    "\n",
    "    def fit(self, df: pd.DataFrame, batch_size: int = 100000):\n",
    "        for start in range(0, len(df), batch_size):\n",
    "            chunk = df.iloc[start:start+batch_size]\n",
    "            self.partial_fit(chunk)\n",
    "        return self\n",
    "\n",
    "    def recommend(self, user_id: str, top_k: int = 10):\n",
    "        return [i for i, _ in self.counter.most_common(top_k)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e032e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDRecommender:\n",
    "    def __init__(self, n_factors=50, n_epochs=20, lr_all=0.005, reg_all=0.02):\n",
    "        self.n_factors = n_factors\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr_all = lr_all\n",
    "        self.reg_all = reg_all\n",
    "        self.model = SVD(n_factors=n_factors, n_epochs=1, lr_all=lr_all, reg_all=reg_all)  \n",
    "        self.trainset = None\n",
    "        self.user_item_map = None  \n",
    "\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        df_tmp = df.copy()\n",
    "        if \"rating\" not in df_tmp.columns:\n",
    "            df_tmp[\"rating\"] = 1.0\n",
    "\n",
    "        reader = Reader(rating_scale=(0, 1))\n",
    "        data = Dataset.load_from_df(df_tmp[[\"user_id\", \"item_id\", \"rating\"]], reader)\n",
    "        self.trainset = data.build_full_trainset()\n",
    "        for epoch in tqdm(range(self.n_epochs), desc=\"Training SVD\"):\n",
    "            self.model.train(self.trainset)\n",
    "\n",
    "        self.user_item_map = df.groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "        return self\n",
    "\n",
    "    def recommend(self, user_id: str, top_k: int = 10):\n",
    "        if self.trainset is None:\n",
    "            return []\n",
    "\n",
    "        if user_id not in self.trainset._raw2inner_id_users:\n",
    "            return []  # cold-start user\n",
    "\n",
    "        inner_uid = self.trainset.to_inner_uid(user_id)\n",
    "        all_items = self.trainset.all_items()\n",
    "        rated = self.trainset.ur[inner_uid]  \n",
    "        rated_items = {iid for iid, _ in rated}\n",
    "\n",
    "        preds = []\n",
    "        for iid in all_items:\n",
    "            if iid not in rated_items:  \n",
    "                est = self.model.predict(user_id, self.trainset.to_raw_iid(iid)).est\n",
    "                preds.append((iid, est))\n",
    "\n",
    "        preds_sorted = sorted(preds, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        return [self.trainset.to_raw_iid(iid) for iid, _ in preds_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d5e1517",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_IMPLICIT:\n",
    "    from scipy.sparse import csr_matrix\n",
    "    from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "    class ALSRecommender:\n",
    "        def __init__(self, factors=50):\n",
    "            self.factors = factors\n",
    "            self.model = AlternatingLeastSquares(factors=factors, iterations=15, use_gpu=False, calculate_training_loss=True, num_threads=4)\n",
    "\n",
    "        def fit(self, df: pd.DataFrame):\n",
    "            rows = df[\"user_id\"].map(user_to_idx)\n",
    "            cols = df[\"item_id\"].map(item_to_idx)\n",
    "            data = np.ones(len(df))\n",
    "            mat = csr_matrix((data, (rows, cols)), shape=(len(user_ids), len(item_ids)))\n",
    "            self.model.fit(mat, show_progress=True, batch_size=10000)\n",
    "            return self\n",
    "\n",
    "        def recommend(self, user_id: int, top_k: int = 10):\n",
    "            if user_id not in user_to_idx:\n",
    "                return []\n",
    "            recs, _ = self.model.recommend(user_to_idx[user_id], csr_matrix((1, len(item_ids))), N=top_k)\n",
    "            return [idx_to_item[i] for i in recs]\n",
    "\n",
    "        def save(self, path: str):\n",
    "            joblib.dump(self.model, path)\n",
    "\n",
    "        def load(self, path: str):\n",
    "            self.model = joblib.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f1b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRecommender:\n",
    "    def __init__(self):\n",
    "        self.G = nx.Graph()\n",
    "\n",
    "    def fit(self, df: pd.DataFrame, batch_size: int = 100000):\n",
    "        for start in range(0, len(df), batch_size):\n",
    "            chunk = df.iloc[start:start+batch_size]\n",
    "            for r in chunk.itertuples():\n",
    "                self.G.add_edge(f\"u_{r.user_id}\", f\"i_{r.item_id}\")\n",
    "        return self\n",
    "\n",
    "    def recommend(self, user_id: int, top_k: int = 10):\n",
    "        start_node = f\"u_{user_id}\"\n",
    "        if start_node not in self.G:\n",
    "            return []\n",
    "        pr = nx.pagerank(self.G, alpha=0.85, personalization={start_node: 1})\n",
    "        recs = [n for n in sorted(pr, key=pr.get, reverse=True) if n.startswith(\"i_\")]\n",
    "        return [int(n[2:]) for n in recs[:top_k]]\n",
    "\n",
    "    def save(self, path: str):\n",
    "        nx.write_gpickle(self.G, path)\n",
    "\n",
    "    def load(self, path: str):\n",
    "        self.G = nx.read_gpickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3834d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_cold(item_id: int, top_k: int = 10):\n",
    "    if item_id not in books_pd[\"item_id\"].values:\n",
    "        return []\n",
    "    genre = books_pd.loc[books_pd[\"item_id\"] == item_id, \"genre\"].values[0]\n",
    "    candidates = books_pd[books_pd[\"genre\"] == genre][\"item_id\"].tolist()\n",
    "    return candidates[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be032d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(pred: List[int], true: List[int], k: int = 10) -> float:\n",
    "    return len(set(pred[:k]) & set(true)) / k\n",
    "\n",
    "def recall_at_k(pred: List[int], true: List[int], k: int = 10) -> float:\n",
    "    return len(set(pred[:k]) & set(true)) / max(1, len(true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23a5d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_models(train_df, test_df, models, top_k=10, save_dir=\"models\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    results = []\n",
    "\n",
    "    def ensure_list(x):\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            return list(x)\n",
    "        if isinstance(x, np.ndarray):\n",
    "            return x.tolist()\n",
    "        return [x]\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        model.fit(train_df)\n",
    "\n",
    "        precisions, recalls = [], []\n",
    "        for row in tqdm(test_df.itertuples(), total=len(test_df), desc=f\"Evaluating {name}\"):\n",
    "            uid = row.user_id\n",
    "            true_items = ensure_list(row.item_id)\n",
    "            preds = model.recommend(uid, top_k=top_k)\n",
    "            precisions.append(precision_at_k(preds, true_items, top_k))\n",
    "            recalls.append(recall_at_k(preds, true_items, top_k))\n",
    "\n",
    "        results.append({\n",
    "            \"model\": name,\n",
    "            \"precision@k\": np.mean(precisions),\n",
    "            \"recall@k\": np.mean(recalls)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d128b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flexonafft/curecsys/myenv/lib/python3.9/site-packages/implicit/cpu/als.py:95: RuntimeWarning: OpenBLAS is configured to use 10 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training popularity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating popularity: 100%|██████████| 185828/185828 [03:27<00:00, 895.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training svd...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m HAS_IMPLICIT:\n\u001b[1;32m      7\u001b[0m     models[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mals\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ALSRecommender(factors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mevaluate_models\u001b[0;34m(train_df, test_df, models, top_k, save_dir)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     precisions, recalls \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tqdm(test_df\u001b[38;5;241m.\u001b[39mitertuples(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_df), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mSVDRecommender.fit\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, mat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[1;32m     17\u001b[0m     batch \u001b[38;5;241m=\u001b[39m mat[start:start\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# после обучения — вычисляем факторы\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_factors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvd\u001b[38;5;241m.\u001b[39mtransform(mat\u001b[38;5;241m.\u001b[39mtoarray())\n",
      "File \u001b[0;32m~/curecsys/myenv/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/curecsys/myenv/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:362\u001b[0m, in \u001b[0;36mIncrementalPCA.partial_fit\u001b[0;34m(self, X, y, check_input)\u001b[0m\n\u001b[1;32m    351\u001b[0m     mean_correction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\n\u001b[1;32m    352\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m/\u001b[39m n_total_samples) \u001b[38;5;241m*\u001b[39m n_samples\n\u001b[1;32m    353\u001b[0m     ) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m-\u001b[39m col_batch_mean)\n\u001b[1;32m    354\u001b[0m     X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(\n\u001b[1;32m    355\u001b[0m         (\n\u001b[1;32m    356\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingular_values_\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents_,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m         )\n\u001b[1;32m    360\u001b[0m     )\n\u001b[0;32m--> 362\u001b[0m U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m U, Vt \u001b[38;5;241m=\u001b[39m svd_flip(U, Vt, u_based_decision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    364\u001b[0m explained_variance \u001b[38;5;241m=\u001b[39m S\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m (n_total_samples \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/curecsys/myenv/lib/python3.9/site-packages/scipy/linalg/_decomp_svd.py:141\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    137\u001b[0m lwork \u001b[38;5;241m=\u001b[39m _compute_lwork(gesXd_lwork, a1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], a1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    138\u001b[0m                        compute_uv\u001b[38;5;241m=\u001b[39mcompute_uv, full_matrices\u001b[38;5;241m=\u001b[39mfull_matrices)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# perform decomposition\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m u, s, v, info \u001b[38;5;241m=\u001b[39m \u001b[43mgesXd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_uv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_uv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_matrices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD did not converge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"popularity\": PopularityRecommender(),\n",
    "    \"svd\": SVDRecommender(n_components=50),\n",
    "    \"graph\": GraphRecommender(),\n",
    "}\n",
    "if HAS_IMPLICIT:\n",
    "    models[\"als\"] = ALSRecommender(factors=50)\n",
    "\n",
    "results = evaluate_models(train_pd, test_pd, models, top_k=10, save_dir=\"models\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
